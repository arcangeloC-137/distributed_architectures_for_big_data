{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "careful-orleans",
   "metadata": {},
   "source": [
    "## SubtractByKey \n",
    "I create a new RDD contaning only the pairs associated with keys that are not in the second RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "headed-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "nameAge = [('Paolo',40), ('Giorgio',22), ('Francesco',35), ('Bruno', 16)]\n",
    "namePhrase = [('Paolo','spam'), ('Giorgio','ciao'), ('Donato','mia'), ('Stefano', 'tua')]\n",
    "\n",
    "nameAgeRDD = sc.parallelize(nameAge)\n",
    "namePhraseRDD = sc.parallelize(namePhrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "historical-ability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bruno', 16), ('Francesco', 35)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subNameAgeRDD = nameAgeRDD.subtractByKey(namePhraseRDD)\n",
    "subNameAgeRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-ambassador",
   "metadata": {},
   "source": [
    "## Join \n",
    "Pay Attention, a lot of data are sent on the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hollywood-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "nameAge = [('Paolo',40), ('Giorgio',22), ('Francesco',35), ('Bruno', 16)]\n",
    "namePhrase = [('Paolo','spam'), ('Giorgio','ciao'), ('Donato','mia'), ('Stefano', 'tua')]\n",
    "\n",
    "nameAgeRDD = sc.parallelize(nameAge)\n",
    "namePhraseRDD = sc.parallelize(namePhrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "boolean-rocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Giorgio', (22, 'ciao')), ('Paolo', (40, 'spam'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinNameAgeRDD = nameAgeRDD.join(namePhraseRDD)\n",
    "joinNameAgeRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "continent-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Join example\n",
    "\n",
    "questions = [(1, 'What is ...?'), (2, 'Who is ...?')]\n",
    "answers = [(1, 'It a car'), (1, 'It is a bike'), (2, 'She is Jenny')]\n",
    "\n",
    "questionsRDD = sc.parallelize(questions)\n",
    "answersRDD = sc.parallelize(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "geographic-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('What is ...?', 'It a car')),\n",
       " (1, ('What is ...?', 'It is a bike')),\n",
       " (2, ('Who is ...?', 'She is Jenny'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinQ = questionsRDD.join(answersRDD)\n",
    "joinQ.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-authentication",
   "metadata": {},
   "source": [
    "## Cogroup\n",
    "Creates tuple with same key and list of values associated with that key in both RDDs. Use it only if you want to retrieve all values associated with first and second RDD same key, to then perform a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acquired-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstList = [(1,'Star Trek'), (1,'Forrest Gump'),( 2,'Forrest Gump')]\n",
    "secondList = [(1,'Woody Allen'), (2,'Quentin Tarantino'), (2,'Alfred Hitchcock')]\n",
    "\n",
    "firstListRDD = sc.parallelize(firstList)\n",
    "secondListRDD = sc.parallelize(secondList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "instructional-german",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f688ca660d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f688ca66350>)),\n",
       " (2,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f688ca66310>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f688ca66c50>))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cogroupRDD = firstListRDD.cogroup(secondListRDD)\n",
    "cogroupRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-trick",
   "metadata": {},
   "source": [
    "## Double RDD actions\n",
    "These are all actions, the only advantage with respect to custom implementations is in compactness. A disadvantage is that, in the following example, the program will execute 6 different actions on the same RDD. This means we are reading input data 6 times. THIS IS SHIT! (use cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "serious-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputRDD = sc.parallelize([1.5,3.5,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "invalid-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum:  7.0\n",
      "Mean:  2.3333333333333335\n",
      "Standard Deviation:  0.8498365855987975\n",
      "Variance:  0.7222222222222222\n",
      "Max:  3.5\n",
      "Min:  1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum: \", inputRDD.sum())\n",
    "print(\"Mean: \", inputRDD.mean())\n",
    "print(\"Standard Deviation: \", inputRDD.stdev())\n",
    "print(\"Variance: \", inputRDD.variance())\n",
    "print(\"Max: \", inputRDD.max())\n",
    "print(\"Min: \", inputRDD.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-basement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
