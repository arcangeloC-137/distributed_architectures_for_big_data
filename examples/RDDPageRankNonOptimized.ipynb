{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Ranking (not optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.42.4.176:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0-cdh6.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function print some statistics about the input RDD\n",
    "def analyzePartitions(myRDD):\n",
    "    if myRDD.partitioner is None:\n",
    "        print(\"Partioner: No partitioner\")\n",
    "    else:\n",
    "        print(\"Partioner: \"+str(myRDD.partitioner.partitionFunc))\n",
    "    \n",
    "    print(\"Num. partitions: \"+ str(myRDD.getNumPartitions()))\n",
    "    \n",
    "    # Create a local copy of the input partitions in a local Python list\n",
    "    partitions = myRDD.glom().collect()\n",
    "\n",
    "    print(\"Content of the partitions\")\n",
    "    for p in partitions:\n",
    "        print(str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input file with the structure of the web graph\n",
    "inputData = sc.textFile(\"./databases/links.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzePartitions(inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format of each input line\n",
    "# PageId,LinksToOtherPages\n",
    "# e.g., P3 [P1,P2,P4,P5]\n",
    "def mapToPairPageIDLinks(line):\n",
    "    fields = line.split(' ')\n",
    "    pageID = fields[0]\n",
    "    links = fields[1].split(',')\n",
    "       \n",
    "    \n",
    "    return (pageID, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = inputData.map(mapToPairPageIDLinks).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzePartitions(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize each page's rank to 1.0; since we use mapValues, \n",
    "# the resulting RDD will have the same partitioner as links\n",
    "ranks = links.mapValues(lambda v: 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzePartitions(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageRankLinks = links.join(ranks)\n",
    "#pageRankLinks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contributions = pageRankLinks.flatMap(computeContributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contributions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a set of pairs from each input pair\n",
    "# input pair: \n",
    "#---- (pageid, (linked pages, current page rank of pageid) )\n",
    "# output pairs:\n",
    "# --- one output pair for each linked page\n",
    "# --- (pageid linked page, current page rank of the linking page pageid / number of linked pages)\n",
    "def computeContributions(pageIDLinksPageRank):\n",
    "    pagesContributions = []\n",
    "    \n",
    "    currentPageRank = pageIDLinksPageRank[1][1] # this takes the 'rank' from the puple ('ID', (['IDs'], 'rank'))\n",
    "    linkedPages = pageIDLinksPageRank[1][0]     # this takes the IDs list from the puple ('ID', (['IDs'], 'rank'))\n",
    "    numLinkedPages = len(linkedPages)\n",
    "    contribution = currentPageRank/numLinkedPages\n",
    "    \n",
    "    for pageidLinkedPage in linkedPages:\n",
    "        pagesContributions.append( (pageidLinkedPage, contribution))\n",
    "    \n",
    "    return pagesContributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 30 iterations of PageRank\n",
    "for x in range(50):\n",
    "    # Retrieve for each page its current pagerank and the list of linked pages\n",
    "    pageRankLinks = links.join(ranks)\n",
    "    # Compute contributions from linking pages to linked pages for this iteration\n",
    "    contributions = pageRankLinks.flatMap(computeContributions)\n",
    "    # Update current pagerank of all pages for this iteration\n",
    "    ranks = contributions.reduceByKey(lambda contribution1, contribution2: contribution1+contribution2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PAY ATTENTION:** we know that RDDs are immutable. This means that we are creating 50 times a new RDD. At the end we will have a reference of the last RDD. This is not very smart or efficient. Using partition by we can do something more efficient. If we apply partition by to 'links', the system will create a new version of this RDD where the data are already organized by key, so the system know that in one specific server you have all the key/val pairs associated with a specific page ID, in another server you'll have some others IDs and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P5', 0.08643208585140724),\n",
       " ('P3', 0.0560163446251082),\n",
       " ('P1', 4.736656637694964),\n",
       " ('P2', 0.014794913519180311),\n",
       " ('P4', 0.10610001830934136)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:18.301273\n"
     ]
    }
   ],
   "source": [
    "end = datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
