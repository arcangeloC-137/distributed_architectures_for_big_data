{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "planned-gazette",
   "metadata": {},
   "source": [
    "## Cache and Persistance in storage\n",
    "\n",
    "It is foundamental to speed up spark executions to use cache and peristence. We can have multiple **Storage Levels**, like:\n",
    "1. MEMORY_ONLY\n",
    "2. MEMORY_AND_DISK\n",
    "3. DISK_ONLY\n",
    "4. MEMORY_ONLY_2 or DISK_ONLY_2 (which replicate each parition on two cluster nodes, if one fails, the other is ready to perform the actions on the RDD)\n",
    "5. OFF_HEAP (experimental)\n",
    "\n",
    "**NOTE:** both cache and persist return a NEW RDD\n",
    "\n",
    "**NOTE 2:** using cache() is the same as using inRDD.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "You can remove RDD from cache by using **unpersist()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of exam question\n",
    "inputRDD = sc.textFile(\"words.txt\")\n",
    "\n",
    "print(\"Num of word: \", inputRDD.count()) # first time\n",
    "print(\"Num of distinct words: \", inputRDD.count()) # second time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-target",
   "metadata": {},
   "source": [
    "**How many times the system will read the content of words.txt?**\n",
    "    Two times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of exam question\n",
    "inputRDD = sc.textFile(\"words.txt\").cache()\n",
    "\n",
    "print(\"Num of word: \", inputRDD.count()) # first time\n",
    "print(\"Num of distinct words: \", inputRDD.count()) # second time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-framing",
   "metadata": {},
   "source": [
    "**And now?** Only one time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-tournament",
   "metadata": {},
   "source": [
    "**NOTE:** If the size of the file is too much for the main memory, cache will read on time plus something that cannot be stored in main memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-pattern",
   "metadata": {},
   "source": [
    "## Accumulators\n",
    "Accumulators are shared variables that are only \"added\" to throygh an associative operation and can therefore be efficientrly supported in parallel. They are mainly used to compute simple statistics like counts or sums. The advantage is that you can transform your data while computing the statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "homeless-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulator example\n",
    "# we want to select lines with valid email (valid if contains @)\n",
    "# we also want on the std output the number of valid emails\n",
    "\n",
    "# We can use two approaches: filters and counters (two Actions), using accumulators\n",
    "sc\n",
    "emails = ['arcangelo@frigiola.it', 'leonardo@maggio.com', 'giuseppe#esposito', 'mario@rossi.it', 'ayeye+brazorf']\n",
    "invalidEmails = sc.accumulator(0)\n",
    "emailsRDD = sc.parallelize(emails)\n",
    "\n",
    "# Define filtering functions\n",
    "def validEmailFunc(line):\n",
    "    if (line.find('@')<0): \n",
    "        invalidEmails.add(1)\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# Select only valid emails\n",
    "# Count also the number of invalid emails \n",
    "validEmailsRDD = emailsRDD.filter(validEmailFunc).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-living",
   "metadata": {},
   "source": [
    "**Question: if we switch A and B, is the result the same?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mechanical-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid email addresses:  2\n"
     ]
    }
   ],
   "source": [
    "# A: Store valid emails in the output file \n",
    "validEmailsRDD.saveAsTextFile('res_accumulators/')\n",
    "\n",
    "# B: Print the number of invalid emails\n",
    "print(\"Invalid email addresses: \", invalidEmails.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-afternoon",
   "metadata": {},
   "source": [
    "No! If we switch A and B to B and A, the print will be Zero! saveAsTextFile is calling our function, so if we first call print, it won't be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scenic-denver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid email addresses:  3\n",
      "Invalid email addresses:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Valid email addresses: \", validEmailsRDD.count())\n",
    "print(\"Invalid email addresses: \", invalidEmails.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collected-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid email addresses:  2\n",
      "Valid email addresses:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Invalid email addresses: \", invalidEmails.value)\n",
    "print(\"Valid email addresses: \", validEmailsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-battlefield",
   "metadata": {},
   "source": [
    "**If we don't use cache istead, the last prints will provide erroneous outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "linear-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the same thing but using foreach+accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amended-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = ['arcangelo@frigiola.it', 'leonardo@maggio.com', 'giuseppe#esposito', 'mario@rossi.it', 'ayeye+brazorf']\n",
    "invalidEmails = sc.accumulator(0)\n",
    "emailsRDD = sc.parallelize(emails)\n",
    "\n",
    "# Define filtering function\n",
    "def invalidEmailFunc(line):\n",
    "    if (line.find('@')<0):\n",
    "        invalidEmails.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "canadian-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "emailsRDD.foreach(invalidEmailFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "similar-necklace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid email addresses:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Invalid email addresses: \", invalidEmails.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-gateway",
   "metadata": {},
   "source": [
    "**NOTE:** Pay attention on the number of actions executed, otherwise the result will not be correct using the accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "velvet-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an accumulator and initialize it to 0\n",
    "discardedQuestion = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "possible-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD1 = sc.parallelize([(1,'Q1'),(2,'Q2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "monthly-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filtering function\n",
    "def selectQuestion(pair):\n",
    "    if(pair[0]==1):\n",
    "        return True\n",
    "    else:\n",
    "        discardedQuestion.add(1)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "essential-russia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('Q1', 'Q1'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD2 = RDD1.filter(selectQuestion)\n",
    "RDD2.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "automatic-character",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discardedQuestion.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-bulletin",
   "metadata": {},
   "source": [
    "How many times are we applying the filter? Only one... but the filter transformation is applied two times. Second time is when we call the join. **PAY ATTENTION, THIS IS A TYPICAL EXAM QUESTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD2 = RDD1.filter(selectQuestion)\n",
    "RDD1.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-runner",
   "metadata": {},
   "source": [
    "In this case the output will be 1. \n",
    "\n",
    "I have RDD1,\n",
    "I apply the **filter** and I obtain RDD2,\n",
    "then I apply **join** to RDD1.\n",
    "\n",
    "**In this case, how many paths are associated with the filter?**\n",
    "We have two paths, but the filter transformation is included only in one path!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-examination",
   "metadata": {},
   "source": [
    "## Broadcast Variables\n",
    "Broadcast variables are read-only shared variables intantiated in the driver and sent to all worker nodes that use it in one or more Sparl operations. They are stored in the main memory of the executors as well. Broadcast variables limit the amount of data sent on the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "entitled-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "# Create an RDD from a txt file containing a dictionary of pairs (word, int value), one pair for each line.\n",
    "# Suppose the content of this file is large but can be stored in main-memory\n",
    "\n",
    "# Create an RDD from a txt file containing a set of words (a sentence for each line)\n",
    "# Transform the content of the second file mapping each word to an intefer based on the dictionaty of the first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the dictionary\n",
    "dictRDD = sc.textFile('dictionary.txt')\\\n",
    "            .map(lambda line: (line.split(' ')[0], line.split(' ')[1]))\n",
    "\n",
    "# Create a broadcast variable based on the content of dictinoaryRDD\n",
    "# Note: broadcast can be instantiated only passing a local variable, not an RDD\n",
    "dictionary = dictRDD.collectAsMap()\n",
    "\n",
    "# Broadcast dictionary\n",
    "dictionaryBroadcast = sc.broadcast(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the second file \n",
    "textRDD = sc.textFile(\"document.txt\")\n",
    "\n",
    "# Define the function that is used to map strings to integers \n",
    "def myMapFunc(line):\n",
    "    transformedLine=''\n",
    "    \n",
    "    for word in line.split(' '):\n",
    "        intValue = dictionaryBroadcast.value[word] \n",
    "        transformedLine = transformedLine+intValue+' '\n",
    "\n",
    "    return transformedLine.strip()\n",
    "\n",
    "# Map words in textRDD to the corresponding integers and concatenate them\n",
    "mappedTextRDD= textRDD.map(myMapFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store tbe result in the output folder\n",
    "mappedTextRDD.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-slope",
   "metadata": {},
   "source": [
    "Do you think that here brodcast was really needed? Broadcast variable here is not really needed since we have only one transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-bristol",
   "metadata": {},
   "source": [
    "## Broadcast Join\n",
    "Boradcast join is similar to Join implemented in MapReduce for Hadoop. In the latter we said that, if you want to perform the join operatino on two input files, and one of them is small, you can use a Map only Join (one copy of the small file shared for all servers). \n",
    "\n",
    "Join transformation is expensive in terms of execition time and amount of data sent on th network. If one of the two input RDDs of the key/val pairs is small enough to be stored in the main memory we can use a more efficient solution based on a broadcast variable. \n",
    "\n",
    "**ATTENTION:** each key in big file can be associated to one single line in the small file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first input file\n",
    "largeRDD = sc.textFile(\"post.txt\").map(lambda line: (int(line.split(',')[0]), line.split(',')[1]) )\n",
    "\n",
    "# Read the second input file\n",
    "smallRDD = sc.textFile(\"profiles.txt\").map(lambda line: (int(line.split(',')[0]), line.split(',')[1]) )\n",
    "\n",
    "# Broadcast join version\n",
    "# Store the \"small\" RDD in a local python variable in the driver\n",
    "# and broadcast it\n",
    "localSmallTable = smallRDD.collectAsMap()\n",
    "localSmallTableBroadcast = sc.broadcast(localSmallTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for joining a record of the large RDD with the matching\n",
    "# record of the small one\n",
    "\n",
    "def joinRecords(largeTableRecord):\n",
    "    returnedRecords = []\n",
    "    key = largeTableRecord[0]\n",
    "    valueLargeRecord = largeTableRecord[1]\n",
    "    \n",
    "    if key in localSmallTableBroadcast.value:\n",
    "    returnedRecords.append( (key, (valueLargeRecord,\\\n",
    "    localSmallTableBroadcast.value[key]) ) )\n",
    "    \n",
    "    return returnedRecords\n",
    "\n",
    "# Execute the broadcast join operation by using a flatMap\n",
    "# transformation on the \"large\" RDD\n",
    "userPostProfileRDDBroadcatJoin = largeRDD.flatMap(joinRecords) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-hampshire",
   "metadata": {},
   "source": [
    "**NOTE:** We use flat map instead of Map since the function **joinRecords** return a single record everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-forward",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
