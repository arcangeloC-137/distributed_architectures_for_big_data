{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "center-spiritual",
   "metadata": {},
   "source": [
    "# ***Spark Structured Streaming***\n",
    "\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine that is built on the Spark SQL engine\n",
    "\n",
    "- Input data are represented by means of (streaming) DataFrames\n",
    "- Structured Streaming uses the existing Spark SQL APIs to query data streams\n",
    "- The same methods we used for analyzing “static” DataFrames\n",
    "    - A set of specific methods that are used to define\n",
    "    - Input and output streams\n",
    "    - Windows\n",
    "    \n",
    "Each input data stream is modeled as a table that is being continuously appended. The expressed queries are incremental queries that are run incrementally on the unbounded input tables. We consider all the data.\n",
    "\n",
    " - The arrive of new data triggers the execution of the incremental queries\n",
    " - The result of a query at a specific timestamp is the one obtained by running the query on all the data arrived until that timestamp\n",
    " \n",
    "\n",
    "The queries can be executed\n",
    "\n",
    "- As micro-batch queries with a fixed batch interval\n",
    "    \n",
    "    - Standard behavior\n",
    "    \n",
    "    - Exactly-once fault-tolerance guarantees\n",
    "- As continuous queries\n",
    "    \n",
    "    - Experimental\n",
    "    \n",
    "    - At-least-once fault-tolerance guarantees\n",
    "    \n",
    "    \n",
    "### **Input Sources**\n",
    "\n",
    "- File source\n",
    "    \n",
    "    - Reads files written in a directory as a stream of data\n",
    "    \n",
    "    - Each line of the input file is an input record\n",
    "    \n",
    "    - Supported file formats are text, csv, json, orc, parquet, ..\n",
    "\n",
    "- Kafka source\n",
    "    \n",
    "    - Reads data from Kafka\n",
    "    \n",
    "    - Each Kafka message is one input record\n",
    "    \n",
    "    \n",
    "The **readStream** property of the SparkSession class is used to create DataStreamReaders\n",
    "- The methods format() and option() of the DataStreamReader class are used to specify the input streams\n",
    "- Type, location, ...\n",
    "- The method load() of the DataStreamReader class is used to return DataFrames associated with the input data stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case I have a DataFrame which \n",
    "# has one record for each input stream\n",
    "recordsDF = spark.readStream \\\n",
    ".format(\"socket\") \\\n",
    ".option(\"host\", \"localhost\") \\\n",
    ".option(\"port\", 9999) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-indiana",
   "metadata": {},
   "source": [
    "### **Transformations**\n",
    "Transformations applied on DataFrames can also be applied to DF o streams. However, there are restrictions on some types of queries/transformations that cannot be executed incrementally.\n",
    "\n",
    "Unsupported operations:\n",
    "\n",
    "- Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DataFrame)\n",
    "\n",
    "- Limit and take first N rows\n",
    "\n",
    "- Distinct operations\n",
    "\n",
    "- Sorting operations are supported on streaming DataFrames only after an aggregation and in complete output mode\n",
    "\n",
    "- Few types of outer joins on streaming DataFrames are not supported\n",
    "\n",
    "\n",
    "## **Ouputs**\n",
    "\n",
    "**Sinks**\n",
    "- They are instances of the class DataStreamWriter and are used to specify the external destinations and store the results in the external destinations\n",
    "\n",
    "**File sink**\n",
    "\n",
    "- Stores the output to a directory\n",
    "\n",
    "- Supported file formats are text, csv, json, orc,\n",
    "parquet, ..\n",
    "\n",
    "**Kafka sink**\n",
    "\n",
    "- Stores the output to one or more topics in Kafka\n",
    "\n",
    "**Foreach sink**\n",
    "\n",
    "- Runs arbitrary computation on the output records\n",
    "\n",
    "**Console sink (for debugging purposes)**\n",
    "- Prints the computed output to the console every time a new batch of records has been analyzed\n",
    "\n",
    "\n",
    "We must define how we want Spark to write output data in the external destinations\n",
    "\n",
    "Supported output modes:\n",
    "\n",
    "- Append\n",
    "\n",
    "- Complete\n",
    "\n",
    "- Update\n",
    "\n",
    "The supported output modes depend on the \n",
    "query type\n",
    "\n",
    "### Append mode\n",
    "\n",
    "Default mode. Only the new rows added to the computed result since the last trigger (computation) will be outputted. Queries with only select, filter, map, flatMap, filter, join, etc. support append mode. So only  on operations that do not need to consider previous data to make a decision.\n",
    "\n",
    "**N.B.**: if you don't use this approach, you need to store the entire result.\n",
    "\n",
    "### Complete mode\n",
    "The whole computed result will be outputted to the sink after every trigger (computation). This mode is supported for aggregation queries. \n",
    "\n",
    "### Update mode\n",
    "Only the rows in the computed result that were updated since the last trigger (computation) will be outputted.\n",
    "\n",
    "## **Output write**\n",
    "The writeStream property of the SparkSession class is used to create DataStreamWriters\n",
    "\n",
    "- The methods outputMode(), format() and option() of the DataStreamWriter class are used to specify the output destination.\n",
    "\n",
    "- Data format, location, output mode, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamWriterRes = stationIdTimestampDF \\\n",
    ".writeStream \\\n",
    ".outputMode(\"append\") \\\n",
    ".format(\"console\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-confirmation",
   "metadata": {},
   "source": [
    "To start executing the defined queries/structured streaming applications you must explicitly invoke the **start()** action on the defined sinks.\n",
    "\n",
    "You can start several queries in the same application. Structured streaming queries run forever. You must explicitly stop/kill them.\n",
    "\n",
    "\n",
    "## **Triggers**\n",
    "\n",
    "For each Spark structured streaming query we can specify when new input data must be processed and whether the query is going to be executed:\n",
    "\n",
    "- as a micro-batch query with a fixed batch interval\n",
    "\n",
    "- or as a continuous processing query (experimental)\n",
    "\n",
    "The trigger type for each query is specified by means of the **trigger()** method of the DataStreamWriter class.\n",
    "\n",
    "If **no trigger** type is explicitly specified, system will try to give an answer every time it has enough data:\n",
    "\n",
    "- Default trigger setting\n",
    "\n",
    "- The query will be executed in micro-batch mode\n",
    "\n",
    "- Each micro-batch is generated and processed as soon as the previous micro-batch has been processed\n",
    "\n",
    "\n",
    "\n",
    "**Fixed interval micro-batches**:\n",
    "\n",
    "- The query will be executed in micro-batch mode\n",
    "\n",
    "- Micro-batches will be processed at the user- specified intervals\n",
    "    \n",
    "    - The parameter processingTime of the trigger method() is used to specify the micro-batch size.\n",
    "    \n",
    "    - If the previous micro-batch completes within its interval, then the engine will wait until the interval is over before processing the next micro-batch.\n",
    "    \n",
    "    - if the previous micro-batch takes longer than the interval to complete (i.e. if an interval boundary is missed), then the next micro-batch will start as soon as the previous one completes\n",
    "    \n",
    "\n",
    "**One-time micro-batch**:\n",
    "\n",
    "- The query will be executed in micro-batch mode\n",
    "\n",
    "- But the query will be executed only one time on one single micro-batch containing all the available data of the input stream:\n",
    "    - After the single execution the query stops on its own\n",
    "\n",
    "- This trigger type is useful when you want to periodically spin up a cluster, process everything that is available since the last period, and then shutdown the cluster.\n",
    "\n",
    "\n",
    "## **Example 1**\n",
    "\n",
    "Problem specification\n",
    "\n",
    "Input\n",
    "\n",
    "- A stream of records retrieved from localhost:9999\n",
    "\n",
    "- Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp\n",
    "\n",
    "- Each input reading has the format\n",
    "\n",
    "- stationId,# free slots,#used slots,timestamp\n",
    "\n",
    "Output\n",
    "\n",
    "- For each input reading with a number of free slots equal to 0 print on the standard output the value of stationId and timestamp\n",
    "\n",
    "- Use the standard micro-batch processing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conscious-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Create a \"receiver\" DataFrame that will connect to localhost:9999\n",
    "# INPUT IS ONE SINGLE STREAM, ONE SINGLE COLUMN!\n",
    "recordsDF = spark.readStream\\\n",
    ".format(\"socket\") \\\n",
    ".option(\"host\", \"localhost\") \\\n",
    ".option(\"port\", 9999) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input records are characterized by one single column called value\n",
    "# of type string\n",
    "# Example of an input record: s1,0,3,2016-03-11 09:00:04\n",
    "# Define four more columns by splitting the input column value\n",
    "# New columns:\n",
    "# - stationId\n",
    "# - freeslots\n",
    "# - usedslots\n",
    "# - timestamp\n",
    "\n",
    "readingsDF = recordsDF\\\n",
    ".withColumn(\"stationId\", split(recordsDF.value, ',')[0].cast(\"string\"))\\\n",
    ".withColumn(\"freeslots\", split(recordsDF.value, ',')[1].cast(\"integer\"))\\\n",
    ".withColumn(\"usedslots\", split(recordsDF.value, ',')[2].cast(\"integer\"))\\\n",
    ".withColumn(\"timestamp\", split(recordsDF.value, ',')[3].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "# Use the standard filter transformation\n",
    "fullReadingsDF = readingsDF.filter(\"freeslots=0\")\n",
    "\n",
    "# Select stationid and timestamp\n",
    "# Use the standard select transformation\n",
    "stationIdTimestampDF = fullReadingsDF.select(\"stationId\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of the structured streaming query will be stored/printed on\n",
    "# the console \"sink“.\n",
    "# append output mode\n",
    "queryFilterStreamWriter = stationIdTimestampDF \\\n",
    ".writeStream \\\n",
    ".outputMode(\"append\") \\\n",
    ".format(\"console\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the execution of the query (it will be executed until it is explicitly stopped)\n",
    "queryFilter = queryFilterStreamWriter.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-religion",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Input\n",
    "\n",
    "- A stream of records retrieved from localhost:9999\n",
    "\n",
    "- Each input record is a reading about the status of a station of\n",
    "\n",
    "- bike sharing system in a specific timestamp\n",
    "\n",
    "- Each input reading has the format:\n",
    "                     [ stationId, #free_slots, #used_slots, timestamp]\n",
    "\n",
    "Output\n",
    "\n",
    "- For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0\n",
    "\n",
    "- Print the requested information when new data are received by using the standard micro-batch processing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Create a \"receiver\" DataFrame that will connect to localhost:9999\n",
    "recordsDF = spark.readStream\\\n",
    ".format(\"socket\") \\\n",
    ".option(\"host\", \"localhost\") \\\n",
    ".option(\"port\", 9999) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input records are characterized by one single column called value\n",
    "# of type string\n",
    "# Example of an input record: s1,0,3,2016-03-11 09:00:04\n",
    "# Define four more columns by splitting the input column value\n",
    "# New columns:\n",
    "# - stationId\n",
    "# - freeslots\n",
    "# - usedslots\n",
    "# - timestamp\n",
    "readingsDF = recordsDF\\\n",
    ".withColumn(\"stationId\", split(recordsDF.value, ',')[0].cast(\"string\"))\\\n",
    ".withColumn(\"freeslots\", split(recordsDF.value, ',')[1].cast(\"integer\"))\\\n",
    ".withColumn(\"usedslots\", split(recordsDF.value, ',')[2].cast(\"integer\"))\\\n",
    ".withColumn(\"timestamp\", split(recordsDF.value, ',')[3].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "# Use the standard filter transformation\n",
    "fullReadingsDF = readingsDF.filter(\"freeslots=0\")\n",
    "\n",
    "# Count the number of readings with a number of free slots equal to 0\n",
    "# for each stationId\n",
    "# The standard groupBy method is used\n",
    "countsDF = fullReadingsDF\\\n",
    ".groupBy(\"stationId\")\\\n",
    ".agg({\"*\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of the structured streaming query will be stored/printed on\n",
    "# the console \"sink\"\n",
    "# complete output mode\n",
    "# (append mode cannot be used for aggregation queries)\n",
    "queryCountStreamWriter = countsDF \\\n",
    ".writeStream \\\n",
    ".outputMode(\"complete\") \\\n",
    ".format(\"console\")\n",
    "\n",
    "# Start the execution of the query (it will be executed until it is explicitly stopped)\n",
    "queryCount = queryCountStreamWriter.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
