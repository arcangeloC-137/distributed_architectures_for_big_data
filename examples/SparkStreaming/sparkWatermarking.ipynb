{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "swiss-destruction",
   "metadata": {},
   "source": [
    "# ***Watermarking***\n",
    "\n",
    "Watermarking is a feature of Spark that allows the user to specify the threshold of late data, and allows the engine to accordingly clean up old state.\n",
    "\n",
    "Results related to old event-times are not needed in many real streaming applications. They can be dropped to improve the efficiency of the application.\n",
    "\n",
    "Specifically, to run windowed queries for days, it is necessary for the system to bound the amount of intermediate in-memory state it accumulates.\n",
    "\n",
    "This means the system needs to know when an old aggregate can be dropped from the in-memory state because the application is not going to receive late data for that aggregate any more. To enable this, in Spark 2.1, watermarking has been introduced.\n",
    "\n",
    "You can define the watermark of a query by specifying the event time column and the threshold on how late the data is expected to be in terms of event time.\n",
    "\n",
    "\n",
    "# ***Join Operations***\n",
    "\n",
    "Spark Structured Streaming manages also join operations\n",
    "\n",
    "- Between two streaming DataFrames (usually put together the batches associated with the same time slot)\n",
    "\n",
    "- Between a streaming DataFrame and a static DataFrame\n",
    "\n",
    "The result of the streaming join is generated incrementally.\n",
    "\n",
    "Hence, old data must be discarded. You must define watermark thresholds on both input streams such that the engine knows how delayed the input can be and drop old data.\n",
    "\n",
    "The methods **join()** and **withWatermark()** are used to join streaming DataFrames. The join method is similar to the one available for static DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "impressions = spark.readStream. ...\n",
    "clicks = spark.readStream. ...\n",
    "\n",
    "\n",
    "# Apply watermarks on event-time columns\n",
    "impressionsWithWatermark = impressions.withWatermark(\"impressionTime\", \"2 hours\")\n",
    "                                                     \n",
    "clicksWithWatermark = clicks.withWatermark(\"clickTime\", \"3 hours\")\n",
    "\n",
    "# Join with event-time constraints\n",
    "impressionsWithWatermark.join(\n",
    "clicksWithWatermark,\n",
    "expr(\"\"\"\n",
    "clickAdId = impressionAdId AND clickTime >= impressionTime AND\n",
    "clickTime <= impressionTime + interval 1 hour\n",
    "\"\"\") )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
