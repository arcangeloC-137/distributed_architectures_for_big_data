{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wound-conference",
   "metadata": {},
   "source": [
    "# ***Spark Streaming Program***\n",
    "\n",
    "**Define** a Spark Streaming Context object\n",
    "- Define the size of the batches (in seconds) associated with the Streaming context\n",
    "- Specify the input stream and define a DStream based on it\n",
    "- Specify the operations to execute for each batch of data\n",
    "- Use transformations and actions similar to the ones available for “standard” RDDs\n",
    "\n",
    "**Invoke** the start method\n",
    "- To start processing the input stream\n",
    "- Wait until the application is killed or the timeout specified in the application expires\n",
    "- If the timeout is not set and the application is not killed the application will run forever\n",
    "\n",
    "### **Initialize Spark Streaming Context**\n",
    "The Spark Streaming Context is defined by using the **StreamingContext(SparkConf sparkC, Duration batchDuration)** constructor of the class pyspark.streaming.StreamingContext.\n",
    "The batchDuration parameter specifies the “size” of the batches in second.\n",
    "\n",
    "### **Input from TCP socket**\n",
    "A DStream can be associated with the content emitted by a TCP socket. \n",
    "**socketTextStream(String hostname, int port_number)** is used to create a DStream based on the textual content emitted by a TPC socket.\n",
    "\n",
    "### **Input from HDFS folder**\n",
    "A DStream can be associated with the content of an input (HDFS) folder\n",
    "- Every time a new file is inserted in the folder, the content of the file is “stored” in the associated DStream and processed\n",
    "- Pay attention that updating the content of a file does not trigger/change the content of the DStream\n",
    "- textFileStream(String folder) is used to create a DStream based on the content of the input folder\n",
    "\n",
    "**N.B.:** data already in the folder before you start the application will not be analyzed!\n",
    "### **Transformations**\n",
    "Analogously to standard RDDs, also DStreams are characterized by a set of transformations. When applied to DStream objects, transformations return a new DStream Object. The transformation is applied on one batch (RDD) of the input DStream at a time and returns a batch (RDD) of the new DStream:\n",
    "- i.e., each batch (RDD) of the input DStream is associated with exactly one batch (RDD) of the returned DStream\n",
    "Many of the available transformations are the same transformations available for standard RDDs.\n",
    "\n",
    "### **Basic transformations:**\n",
    "- **map(func)**\n",
    "    - Returns a new DStream by passing each element of the source DStream through a function func\n",
    "- **flatMap(func)**\n",
    "    - Each input item can be mapped to 0 or more output items. Returns a new DStream\n",
    "- **filter(func)**\n",
    "    - Returns a new DStream by selecting only the records of the source DStream on which func returns true\n",
    "- **reduce(func)** (in DStreams is no longer an action but a transformation)\n",
    "    - Returns a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func\n",
    "         - The function must be associative and commutative so that it can be computed in parallel\n",
    "    - Note that the reduce method of DStreams is a transformation\n",
    "- **reduceByKey(func)**\n",
    "    - When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function\n",
    "- **combineByKey( createCombiner, mergeValue, mergeCombiners)**\n",
    "    - When called on a DStream of (K, V) pairs, returns a new DStream of (K, W) pairs where the values for each key are aggregated using the given combine functions\n",
    "- **reduceByKey(func)**\n",
    "    - When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function\n",
    "- **combineByKey( createCombiner, mergeValue, mergeCombiners)**\n",
    "    - When called on a DStream of (K, V) pairs, returns a new DStream of (K, W) pairs where the values for each key are aggregated using the given combine functions\n",
    "- **countByValue()**\n",
    "    - When called on a DStream of elements of type K, returns a new DStream of (K, Long) pairs where the value of each key is its frequency in each batch of the source Dstream\n",
    "    - Note that the countByValue method of DStreams is a transformation\n",
    "- **count()**\n",
    "    - Returns a new DStream of single-element RDDs by counting the number of elements in each batch (RDD) of the source Dstream\n",
    "        - i.e., it counts the number of elements in each input batch (RDD)\n",
    "    - Note that the count method of DStreams is a transformation\n",
    "- **union(otherStream)**\n",
    "    - Returns a new DStream that contains the union of the elements in the source DStream and otherDStream\n",
    "- **join(otherStream)**\n",
    "    - When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key\n",
    "- **cogroup(otherStream)**\n",
    "    - When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples\n",
    "\n",
    "### **Basic Actions**\n",
    "- **pprint()**\n",
    "    - Prints the first 10 elements of every batch of data in a DStream on the standard output of the driver node running the streaming application\n",
    "      - Useful for development and debugging\n",
    "- **saveAsTextFiles(prefix, [suffix])**\n",
    "    - Saves the content of the DStream on which it is invoked as text files\n",
    "      - One folder for each batch\n",
    "      - The folder name at each batch interval is generated based on prefix, time of the batch (and suffix): \"prefix-TIME_IN_MS[.suffix]“\n",
    "      \n",
    "\n",
    "### **Start and Run**\n",
    "- The **streamingContext.start()** method is used to start the application on the input stream(s)\n",
    "- The **awaitTerminationOrTimeout(long millisecons)** method is used to specify how long the application will run\n",
    "- The **awaitTermination()** method is used to run the application forever\n",
    "- Until the application is explicitly killed\n",
    "- The processing can be manually stopped using **streamingContext.stop()**\n",
    "\n",
    "**Points to remember:**\n",
    "- Once a context has been started, no new streaming computations can be set up or added to it\n",
    "- Once a context has been stopped, it cannot be restarted\n",
    "- Only one StreamingContext per application can be active at the same time\n",
    "- stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
