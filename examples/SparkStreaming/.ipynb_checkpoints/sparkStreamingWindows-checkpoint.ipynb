{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "after-strengthening",
   "metadata": {},
   "source": [
    "# ***Spark Streaming Window Operation***\n",
    "\n",
    "Spark Streaming also provides windowed computations. It allows you to apply transformations over a sliding window of data:\n",
    "- Each window contains a set of batches of the input stream\n",
    "- Windows can be overlapped\n",
    "    - i.e., the same batch can be included in many consecutive windows\n",
    "    \n",
    "Windows CAN be overlapped, but CANNOT be something in which you have one batch and another. Each window must contain a number of batches that is a multiple of the batch size.\n",
    "\n",
    "Every time the window slides over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream.\n",
    "\n",
    "Any window operation needs to specify two parameters:\n",
    "\n",
    "- **Window length**\n",
    "    - The duration of the window (3 in the example)\n",
    "- **Sliding interval**\n",
    "    - The interval at which the window operation is performed (2 in the example)\n",
    "\n",
    "These two parameters must be multiples of the batch interval of the source DStream.\n",
    "\n",
    "\n",
    "## **Basic Window Transformations**\n",
    "\n",
    "If you want to manage windows, a set of functions are avaliable:\n",
    "\n",
    "- **window(windowLength, slideInterval)**\n",
    "    - Returns a new DStream which is computed based on windowed batches of the source DStream\n",
    "\n",
    "\n",
    "- **countByWindow(windowLength, slideInterval)**\n",
    "    - Returns a new single-element stream containing the number of elements of each window\n",
    "        - The returned object is a Dstream of Long objects. However, it contains only one value for each window (the number of elements of the last analyzed window)\n",
    "\n",
    "\n",
    "- **reduceByWindow(reduceFunc, invReduceFunc, windowDuration, slideDuration)**\n",
    "    - Returns a new single-element stream, created by aggregating elements in the stream over a sliding interval using func\n",
    "        - The function must be associative and commutative so that it can be computed correctly in parallel\n",
    "    - If invReduceFunc is not None, the reduction is done incrementally using the old window's reduced value\n",
    "\n",
    "\n",
    "- **countByValueAndWindow(windowDuratio n , slideDuration)**\n",
    "    - When called on a DStream of elements of type K, returns a new DStream of (K, Long) pairs where the value of each key K is its frequency in each window of the source DStream\n",
    "    \n",
    "\n",
    "- **reduceByKeyAndWindow(func, invFunc, windowDuration, slideDuration=None, numPartitions=None)**\n",
    "    - When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function func over batches in a sliding window\n",
    "    - The window duration (length) is specified as a parameter of this invocation (windowDuration)\n",
    "    - If **slideDuration** is None, the batchDuration of the StreamingContext object is used\n",
    "        - i.e., 1 batch sliding window\n",
    "    - If **invFunc** is provideved (is not None), the reduction is done incrementally using the old window's reduced values\n",
    "        - i.e., invFunc is used to apply an inverse reduce operation by considering the old values that left the window (e.g., subtracting old counts)\n",
    "        \n",
    "        \n",
    "# ***Checkpoints***\n",
    "A streaming application must operate 24/7 and hence must be resilient to failures unrelated to the application logic (e.g., system failures, JVM crashes, etc.) For this to be possible, Spark Streaming needs to checkpoint enough information to a fault- tolerant storage system such that it can recover from failures\n",
    "\n",
    "Checkpointing is enabled by using the **checkpoint(String folder)** method of SparkStreamingContext\n",
    "\n",
    "\n",
    "## **Word Count with Windows and Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Set prefix of the output folders\n",
    "outputPathPrefix=\"resSparkStreamingExamples\"\n",
    "\n",
    "#Create a configuration object and#set the name of the applicationconf\n",
    "SparkConf().setAppName(\"Streaming word count\")\n",
    "\n",
    "# Create a Spark Context object\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Set the checkpoint folder (it is needed by some window transformations)\n",
    "ssc.checkpoint(\"checkpointfolder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Apply a chain of transformations to perform the word count task\n",
    "# The returned RDDs are DStream RDDs\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "wordsOnes = words.map(lambda word: (word, 1))\n",
    "\n",
    "# reduceByKeyAndWindow is used instead of reduceByKey\n",
    "# The durantion of the window is also specified\n",
    "wordsCounts = wordsOnes\\\n",
    ".reduceByKeyAndWindow(lambda v1, v2: v1+v2, None, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-strike",
   "metadata": {},
   "source": [
    "**Typical Exam Question**\n",
    "- Professor shows to code snippets, one with **reduceByKey** and the other with **reduceByKeyAndWindow**, and ask 'Is the output the same or not?'.\n",
    "\n",
    "Basic idea is that, if you apply window, only the window method, you somehow redefine the granularity of you DStream. All the operations applied will be at window level, on the bateches contained in the window. \n",
    "\n",
    "**In general**, if you apply window, and you have more than one analysis associated with the generated window, it is better to window and specific methods without window. \n",
    "\n",
    "**Differently**, if you have a single step associated with that specific window, you can use the approach of this example. Also if the size of the window for different operations are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the num. of occurrences of each word of the current window\n",
    "# (only 10 of them)\n",
    "wordsCounts.pprint()\n",
    "\n",
    "# Store the output of the computation in the folders with prefix\n",
    "# outputPathPrefix\n",
    "wordsCounts.saveAsTextFiles(outputPathPrefix, \"\")\n",
    "\n",
    "#Start the computation\n",
    "ssc.start()\n",
    "ssc.awaitTermination ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-speaking",
   "metadata": {},
   "source": [
    "## **Word count - Version 2**\n",
    "\n",
    "first part is the same ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Apply a chain of transformations to perform the word count task\n",
    "# The returned RDDs are DStream RDDs\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "wordsOnes = words.map(lambda word: (word, 1))\n",
    "\n",
    "# reduceByKeyAndWindow is used instead of reduceByKey\n",
    "# The durantion of the window is also specified\n",
    "wordsCounts = wordsOnes\\\n",
    ".reduceByKeyAndWindow(lambda v1, v2: v1+v2, \\\n",
    "                      lambda vnow, vold: vnow-vold, 15)\n",
    "# In this solution the inverse function is also specified\n",
    "#Â in order to compute the result incrementally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-school",
   "metadata": {},
   "source": [
    "In the **reduceByKeyAndWindow** you can see that there are two set of functions. The first one is used to combine the values that we have. It is used to sum the values associated with the batch in that window. Simply sum the values. \n",
    "\n",
    "The second part recieves the old value and the new one (**vold** is the sum of the values of the first part of the previous window, **vnow** is the new value considering all the data). Applying this function, you are discarding the contribution of the first part of the privious window. \n",
    "\n",
    "Let's visualize it:\n",
    "\n",
    "**first batch**\n",
    "Paolo\n",
    "\n",
    "Paolo\n",
    "\n",
    "Paolo\n",
    "\n",
    "Paolo\n",
    "\n",
    "Garza\n",
    "\n",
    "**second batch**\n",
    "Paolo\n",
    "\n",
    "Garza\n",
    "\n",
    "**third batch**\n",
    "Paolo\n",
    "\n",
    "\n",
    "**first window**\n",
    "If we consider the first window (first + second batch), we'll have:\n",
    "(Paolo, 5)\n",
    "(Graza, 2)\n",
    "\n",
    "If you don't use the inverse function, the system will re-analyze this input data and compute the final result combining the seven value.\n",
    "\n",
    "\n",
    "**second window**\n",
    "For the second window, if we don't use the incremental approach (aka inverse function) system will return:\n",
    "(Paolo, 2)\n",
    "(Garza, 1)\n",
    "\n",
    "In order to do this, the system needs to re-analyze the entire content of the window. \n",
    "\n",
    "### Now, system can do something slightly more efficient\n",
    "\n",
    "To compute these result, system can use another approach. During the analysis of the first window, system can store some intermediate results. Specifically, it can store the infromation that in the first batch we have:\n",
    "\n",
    "(Paolo, 4)\n",
    "\n",
    "(Garza, 1)\n",
    "\n",
    "While in the second batch we have:\n",
    "\n",
    "(Paolo, 1)\n",
    "\n",
    "(Garza, 1)\n",
    "\n",
    "And then the output of the first window:\n",
    "\n",
    "(Paolo, 5)\n",
    "\n",
    "(Garza, 2)\n",
    "\n",
    "Given these values, we can compute the second window by considering the difference between first and second batch, and summing it with the third batch:\n",
    "\n",
    "**first batch - second batch**\n",
    "\n",
    "(Paolo, |5 - 4|)\n",
    "\n",
    "(Garza, |1 - 2|)\n",
    "\n",
    "=\n",
    "\n",
    "\n",
    "(Paolo, 1)\n",
    "\n",
    "(Garza, 1)\n",
    "\n",
    "**intermediate result + third batch**\n",
    "\n",
    "(Paolo, 1 + 1)\n",
    "\n",
    "(Garza, 1 + 0)\n",
    "\n",
    "=\n",
    "\n",
    "\n",
    "(Paolo, 2)\n",
    "\n",
    "(Garza, 1)\n",
    "\n",
    "There is no need to rescan all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-deposit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
