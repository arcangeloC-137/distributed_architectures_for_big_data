{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amazing-dating",
   "metadata": {},
   "source": [
    "# ***Streaming Data Analytics***\n",
    "\n",
    "Act of continuously incorporating new data to compute a result Input data is unbounded → no beginning and no end. The application will output multiple versions of the results as it runs or put them in a storage.\n",
    "\n",
    "Many important applications must process large streams of live data and provide results in near-real-time:\n",
    "- Social network trends\n",
    "- Website statistics\n",
    "- Intrusion detection systems\n",
    "- ...\n",
    "\n",
    "Several frameworks have been proposed to process in real-time or in near real-time data streams:\n",
    "- Apache Spark (Streaming component)\n",
    "- Apache Storm\n",
    "- Amazon Kinensis Streams\n",
    "- ...\n",
    "\n",
    "All these frameworks use a cluster of servers to scale horizontally with respect to the (big) amount of data to be analyzed.\n",
    "\n",
    "Two main “solutions”\n",
    "1. **“Continuous”** computation of data streams\n",
    "    - Data are processed as soon as they arrive\n",
    "        - Every time a new record arrives from the input stream, it is immediately processed and a result is emitted as soon as possible\n",
    "    - Real-time processing\n",
    "2. **“Micro-batch”** stream processing\n",
    "    - Input data are collected in micro-batches\n",
    "    - Each micro-batch contains all the data received in a time window (typically less than a few seconds of data)\n",
    "    - One micro-batch a time is processed\n",
    "    - Every time a micro-batch of data is ready, its entire content is processed and a result is emitted\n",
    "    - Near real-time processing\n",
    "    \n",
    "### **Type of processing**\n",
    "\n",
    "**At-most-once**\n",
    "- Every input element of a stream is processed once or less\n",
    "- It is also called no guarantee\n",
    "- The result **can be wrong/approximated**\n",
    "\n",
    "**At-least-once**\n",
    "- Every input element of a stream is processed once or more\n",
    "- Input elements are replayed when there are failures\n",
    "- The result **can be wrong/approximated**\n",
    "\n",
    "**Exactly-once**\n",
    "- Every input element of a stream is processed exactly once\n",
    "- Input elements are replayed when there are failures\n",
    "- If elements have been already processed they are not reprocessed\n",
    "- The result is **always correct**\n",
    "- Slower than the other processing approaches\n",
    "\n",
    "\n",
    "# ***Spark Streaming***\n",
    "\n",
    "Spark Streaming is a framework for large scale stream processing:\n",
    "- Scales to 100s of nodes\n",
    "- Can achieve second scale latencies\n",
    "- Provides a simple batch-like API for implementing complex algorithm\n",
    "- Micro-batch streaming processing\n",
    "- Exactly-once guarantees\n",
    "- Can absorb live data streams from Kafka, Flume, ZeroMQ, Twitter, ...\n",
    "\n",
    "Spark streaming runs a streaming computation as a series of very small, deterministic batch jobs. It splits each input stream in “portions” andprocesses one portion at a time (in the incoming order). Each **portion** is called **batch**.\n",
    "\n",
    "Spark streaming\n",
    "- Splits the live stream into batches of X seconds\n",
    "- Treats each batch of data as RDDs and processes them using RDD operations\n",
    "- Finally, the processed results of the RDD operations are returned in batches\n",
    "\n",
    "## **Key Concepts**\n",
    "\n",
    "**DStream**\n",
    "- Sequence of RDDs representing a discretized version of the input stream of data\n",
    "    - Twitter, HDFS, Kafka, Flume, ZeroMQ, Akka Actor, TCP sockets, ..\n",
    "- One RDD for each batch of the input stream\n",
    "\n",
    "\n",
    "**Transformations**\n",
    "- Modify data from one DStream to another\n",
    "- “Standard” RDD operations\n",
    "    - map, countByValue, reduce, join, ...\n",
    "    - Window and Stateful operations\n",
    "    - window, countByValueAndWindow, ...\n",
    "    \n",
    "\n",
    "**Output Operations/Actions**\n",
    "- Send data to external entity\n",
    "    - saveAsHadoopFiles, saveAsTextFile, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-investor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
