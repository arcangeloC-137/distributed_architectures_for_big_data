{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "multiple-tunisia",
   "metadata": {},
   "source": [
    "# ***Event Time and Window Operation***\n",
    "\n",
    "It can happen sensors are far from the Data Center where they are processed. In this case, let's suppose we take streaming data from IoT devices, the data from a device 10km far from the DC will be processed after the data arrived from a device 1km far, even if the data from the first were generated before with respect to the latter. Let's see how we overcome this issue.\n",
    "\n",
    "Input streaming records are usually characterized by a time information:\n",
    "- It is the time when the data was generated\n",
    "- It is usually called event-time\n",
    "\n",
    "You want to use the time when the data was generated (i.e., the event-time) rather than the time Spark receives them.\n",
    "\n",
    "Spark allows defining windows based on the time-event input column, and then apply aggregation functions over each window.\n",
    "\n",
    "For each structured streaming query on which you want to apply a window computation you must:\n",
    "\n",
    "- Specify the name of the time-event column in the input (streaming) DataFrame\n",
    "\n",
    "- The characteristics of the (sliding) windows\n",
    "    - windowDuration\n",
    "    - slideDuration\n",
    "    - Do not set it if you want non-overlapped windows\n",
    "    \n",
    "The **window(timeColumn, windowDuration, slideDuration=None)** function is used inside the standard groupBy() one to specify the characteristics of the windows.\n",
    "\n",
    "**N.B.**: Windows can be used only with queries that are applying aggregation functions!\n",
    "\n",
    "\n",
    "### **Example 1**\n",
    "\n",
    "Input\n",
    "\n",
    "- A stream of records retrieved from localhost:9999\n",
    "\n",
    "- Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp\n",
    "\n",
    "- Each input reading has the format\n",
    "                    [ stationId, #free_slots, #used_slots, timestamp]\n",
    "\n",
    "- timestamp is the event-time column\n",
    "\n",
    "Output\n",
    "\n",
    "- For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0 in each window\n",
    "\n",
    "- The query is executed for each window\n",
    "\n",
    "- Set windowDuration to 2 seconds and no slideDuration\n",
    "\n",
    "- i.e., non-overlapped window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Create a \"receiver\" DataFrame that will connect to localhost:9999\n",
    "recordsDF = spark.readStream\\\n",
    ".format(\"socket\") \\\n",
    ".option(\"host\", \"localhost\") \\\n",
    ".option(\"port\", 9999) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input records are characterized by one single column called value\n",
    "# of type string\n",
    "# Example of an input record: s1,0,3,2016-03-11 09:00:04\n",
    "# Define four more columns by splitting the input column value\n",
    "# New columns:\n",
    "# - stationId\n",
    "# - freeslots\n",
    "# - usedslots\n",
    "# - timestamp\n",
    "\n",
    "readingsDF = recordsDF\\\n",
    ".withColumn(\"stationId\", split(recordsDF.value, ',')[0].cast(\"string\"))\\\n",
    ".withColumn(\"freeslots\", split(recordsDF.value, ',')[1].cast(\"integer\"))\\\n",
    ".withColumn(\"usedslots\", split(recordsDF.value, ',')[2].cast(\"integer\"))\\\n",
    ".withColumn(\"timestamp\", split(recordsDF.value, ',')[3].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "# Use the standard filter transformation\n",
    "fullReadingsDF = readingsDF.filter(\"freeslots=0\")\n",
    "\n",
    "# Count the number of readings with a number of free slots equal to 0\n",
    "# for each stationId in each window.\n",
    "# windowDuration = 2 seconds\n",
    "# no overlapping windows\n",
    "countsDF = fullReadingsDF\\\n",
    ".groupBy(window(fullReadingsDF.timestamp, \"2 seconds\"), \"stationId\")\\\n",
    ".agg({\"*\":\"count\"})\\\n",
    ".sort(\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of the structured streaming query will be stored/printed on\n",
    "# the console \"sink\"\n",
    "# complete output mode\n",
    "# (append mode cannot be used for aggregation queries)\n",
    "queryCountWindowStreamWriter = countsDF \\\n",
    ".writeStream \\\n",
    ".outputMode(\"complete\") \\\n",
    ".format(\"console\")\\\n",
    ".option(\"truncate\", \"false“)\n",
    "\n",
    "# Start the execution of the query (it will be executed until it is explicitly stopped)\n",
    "queryCountWindow = queryCountWindowStreamWriter.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-screw",
   "metadata": {},
   "source": [
    "## **Late Events**\n",
    "\n",
    "Sparks handles data that have arrived later than expected based on its event-time. They are called late data.\n",
    "\n",
    "Every time new data are processed the result is computed by combining old aggregate values and the new data by considering the event-time column instead of the time Spark receives the data.\n",
    "\n",
    "The code is the same of “Event Time and Window Operations: Example 3”!\n",
    "\n",
    "**Late data are automatically handled by Spark**\n",
    "\n",
    "### **Example 2**\n",
    "\n",
    "Input\n",
    "\n",
    "- A stream of records retrieved from localhost:9999\n",
    "\n",
    "- Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp\n",
    "\n",
    "- Each input reading has the format\n",
    "                    [ stationId, #free_slots, #used_slots, timestamp]\n",
    "\n",
    "- timestamp is the event-time column\n",
    "\n",
    "Output\n",
    "\n",
    "- For each window, print on the standard output the total number of received input readings with a number of free slots equal to 0\n",
    "\n",
    "- The query is executed for each window\n",
    "\n",
    "- Set windowDuration to 2 seconds and no slideDuration\n",
    "\n",
    "- i.e., non-overlapped windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "# Filter data\n",
    "# Use the standard filter transformation\n",
    "fullReadingsDF = readingsDF.filter(\"freeslots=0\")\n",
    "\n",
    "# Count the number of readings with a number of free slots equal to 0\n",
    "# for each stationId in each window.\n",
    "# windowDuration = 2 seconds\n",
    "# no overlapping windows\n",
    "countsDF = fullReadingsDF\\\n",
    ".groupBy(window(fullReadingsDF.timestamp, \"2 seconds\"))\\\n",
    ".agg({\"*\":\"count\"})\\\n",
    ".sort(\"window\")\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
