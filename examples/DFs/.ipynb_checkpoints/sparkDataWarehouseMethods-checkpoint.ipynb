{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifth-chicago",
   "metadata": {},
   "source": [
    "# Data Warehouse Methods\n",
    "\n",
    "## Cube\n",
    "The method cube(col1, .., coln) of the DataFrame class can be used to create a multi- dimensional cube for the input DataFrame. On top of which aggregate functions can be computed for each “group”.\n",
    "\n",
    "## Rollup\n",
    "The method rollup(col1, .., coln) of the DataFrame class can be used to create a multi- dimensional rollup for the input DataFrame. On top of which aggregate functions can be computed for each “group”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "young-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session object\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read purchases.csv and store it in a DataFrame\n",
    "dfPurchases = spark.read.load(\"./databases/purchases.csv\",format=\"csv\",header=True,inferSchema=True)\n",
    "\n",
    "dfCube = dfPurchases.cube(\"userid\",'productid').agg({\"quantity\": \"sum\"})\n",
    "dfRollup = dfPurchases.rollup(\"userid\",'productid').agg({\"quantity\": \"sum\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-survival",
   "metadata": {},
   "source": [
    "**Cube:** all possible combinations of userId and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accurate-macintosh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userid='u1', productid='p1', sum(quantity)=30),\n",
       " Row(userid=None, productid=None, sum(quantity)=150),\n",
       " Row(userid='u1', productid='p3', sum(quantity)=10),\n",
       " Row(userid=None, productid='p2', sum(quantity)=20),\n",
       " Row(userid='u2', productid='p3', sum(quantity)=70),\n",
       " Row(userid='u1', productid='p2', sum(quantity)=20),\n",
       " Row(userid=None, productid='p1', sum(quantity)=50),\n",
       " Row(userid=None, productid='p3', sum(quantity)=80),\n",
       " Row(userid='u2', productid=None, sum(quantity)=90),\n",
       " Row(userid='u1', productid=None, sum(quantity)=60),\n",
       " Row(userid='u2', productid='p1', sum(quantity)=20)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCube.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-bacon",
   "metadata": {},
   "source": [
    "**Rollup:** combinations where the order is important. In this case we do not have userId at null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coastal-latest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userid='u1', productid='p1', sum(quantity)=30),\n",
       " Row(userid=None, productid=None, sum(quantity)=150),\n",
       " Row(userid='u1', productid='p3', sum(quantity)=10),\n",
       " Row(userid='u2', productid='p3', sum(quantity)=70),\n",
       " Row(userid='u1', productid='p2', sum(quantity)=20),\n",
       " Row(userid='u2', productid=None, sum(quantity)=90),\n",
       " Row(userid='u1', productid=None, sum(quantity)=60),\n",
       " Row(userid='u2', productid='p1', sum(quantity)=20)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRollup.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-controversy",
   "metadata": {},
   "source": [
    "## Set Transformation\n",
    "Similarly to RDDs also DataFrames can be combined by using set transformations:\n",
    " - **df1.union(df2)**, union between dfs, duplicates not removed.\n",
    " - **df1.intersect(df2)**, intersection.\n",
    " - **df1.subtract(df2)**, remove df2 from df1, computed on the whole record, not only the key. Better anti join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-giant",
   "metadata": {},
   "source": [
    "# Broadcast Join\n",
    "If you perform a Broadcast join, system will send a small table on the network only one tume, using a broadcast variable. In this way, the system can execute the join operation more efficiently.\n",
    "Spark SQL automatically implements a broadcast version of the join operation if one of the two input DataFrames is small enough to be stored in the main memory of each executor. \n",
    "\n",
    "We can suggest/force it by creating a broadcast version of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPersonLikesBroadcast = dfUidSports.join(broadcast(dfPersons), dfPersons.uid == dfUidSports.uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-arbitration",
   "metadata": {},
   "source": [
    "# Explain\n",
    "The method explain() can be invoked on a DataFrame to print on the standard output the execution plan of the part of the code that is used to compute the content of the DataFrame on which explain() is invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-leave",
   "metadata": {},
   "source": [
    "# Caching DataFrames\n",
    "Another thing, not present on the slides, that is possible to do is caching a DataFrame. If a DF is used more than one time, associated with more than one action, in that case is possible to invoke **.cache()** on the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-antibody",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
