{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thick-order",
   "metadata": {},
   "source": [
    "## Show\n",
    "Used for debugging to check if you are correctly uploading your DF. It prints the first n values in **.show(n)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session object\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a DataFrame from persons.csv\n",
    "df = spark.read.load( \"persons.csv\", format=\"csv\", header=True, inferSchema=True)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-fortune",
   "metadata": {},
   "source": [
    "## Print Schema\n",
    "Prints on the std ouput the schema of your DataFrame\n",
    "\n",
    "## Count\n",
    "Returns the number of rows in the input DF\n",
    "\n",
    "## Distinct\n",
    "Removes duplicates from DF. Use it only if you really need it, since it is always a hevy operation in terms of data sent on the network. A shuffle phase is indeed needed.\n",
    "\n",
    "## Select\n",
    "The select(col1, .., coln) method of the DataFrame class returns a new DataFrame that contains only the specified columns of the input DataFrame. Use * only to create a copy if the DF since it selects all columns. \n",
    "**PAY ATTENTION:** select can generate errors at runtime if there are mistakes in the names of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from persons2.csv\n",
    "df = spark.read.load( \"persons2.csv\",format=\"csv\",header=True,inferSchema=True)\n",
    "\n",
    "dfNamesAges = df.select(\"name\", \"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-kazakhstan",
   "metadata": {},
   "source": [
    "## SelectExpr\n",
    "The selectExpr(expression1, ..,expressionN) method of the DataFrame class is a variant of the select method, where expr can be an SQL expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from persons.csv\n",
    "df = spark.read.load(\"persons2.csv\", format=\"csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a new DataFrame with four columns:\n",
    "# name, age, gender, newAge = age +1\n",
    "dfNewAge = df.selectExpr(\"name\", \"age\", \"gender\", \"age+1 as newAge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-builder",
   "metadata": {},
   "source": [
    "## Filter\n",
    "The filter(conditionExpr) method of the DataFrame class returns a new DataFrame that contains only the rows satisfying the specified condition. In Scala or Java we have also dataset instead of df, were we can use lambda function to catch errors at compile time, not only at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session object\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a DataFrame from persons.csv\n",
    "df = spark.read.load( \"persons.csv\",format=\"csv\",header=True,inferSchema=True)\n",
    "\n",
    "df_filtered = df.filter(\"age>=20 and age<=31\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-orange",
   "metadata": {},
   "source": [
    "## Where\n",
    "Is equivalent to Filter, they are alias\n",
    "\n",
    "## Join\n",
    "The join(right, on, how) method of the DataFrame class is used to join two DataFrames. It returns a DataFrame that contains the join of the tuples of the two input DataFrames based on the on join condition.\n",
    "\n",
    "**Type of join:**\n",
    "\n",
    "- inner (default)\n",
    "- cross\n",
    "- outer\n",
    "- full\n",
    "- full_outer\n",
    "- left\n",
    "- left_outer\n",
    "- right\n",
    "- right outer\n",
    "- left_semi\n",
    "- left_anti\n",
    "\n",
    "**NOTE:** this method can generate errors at runtime if there are errors in the join expression.\n",
    "\n",
    "### **Example 1**\n",
    "Two DFs:\n",
    "- uid, name, age\n",
    "- uid, sportname\n",
    "\n",
    "Join the content of the two DataFrames (uid is the join column) and show it on the standard output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read persons_id.csv and store it in a DataFrame\n",
    "dfPersons = spark.read.load(\"persons_id.csv\",format=\"csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Read liked_sports.csv and store it in a DataFrame\n",
    "dfUidSports = spark.read.load(\"liked_sports.csv\",format=\"csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Join the two input DataFrames, note that the first param is the 'right' df\n",
    "dfPersonLikes = dfPersons.join(dfUidSports,dfPersons.uid == dfUidSports.uid)\n",
    "\n",
    "# Print the result on the standard output\n",
    "dfPersonLikes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-memory",
   "metadata": {},
   "source": [
    "### **Example 2**\n",
    "Two DFs:\n",
    "- uid, name, age\n",
    "- uid, bannedmotivation\n",
    "\n",
    "Select the profiles of the non-banned users and show them on the standard output. Anti join selects only the rows were a specified parameter is not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read persons_id.csv and store it in a DataFrame\n",
    "dfPersons = spark.read.load(\"persons_id.csv\",format=\"csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Read banned.csv and store it in a DataFrame\n",
    "dfBannedUsers = spark.read.load(\"banned.csv\",format=\"csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Apply the Left Anti Join on the two input DataFrames\n",
    "dfSelectedProfiles = dfPersons.join(dfBannedUsers,dfPersons.uid == dfBannedUsers.uid,\"left_antiâ€œ)\n",
    "\n",
    "# Print the result on the standard output\n",
    "dfSelectedProfiles.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
