{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reliable-structure",
   "metadata": {},
   "source": [
    "## Creating DataFrames from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "periodic-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Create a Spark Session object\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from persons.csv\n",
    "df = spark.read.load(\"persons.csv\",           # input file directory\n",
    "                        format = \"csv\",       # input file fotmat\n",
    "                        header = True,        # specify if the first line contains attributes/column's names\n",
    "                        inferSchema = True)   # specify if system have to infer data types, otherwise all strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-twenty",
   "metadata": {},
   "source": [
    "## Creating DataFrames from JSON file - **Option 1 (not standard JSON file)**\n",
    "**Pay Attention:** Spark SQL privides an API that allows creating a DataFrame directly from txt file where each line contains a JSON obj, hence the input file is not a **standard** JSON file, but must be formatted in order to have one JSON object (tuple) for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from persons.json\n",
    "df = spark.read.load(\"persons.csv\",           # input file directory\n",
    "                        format = \"json\")       # input file fotmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-helena",
   "metadata": {},
   "source": [
    "## Creating DataFrames from JSON file - **Option 2 (standard JSON file)**\n",
    "The same API allows also reading 'standard' multiline JSON files by setting the argument **multiLine = True** on the defined DataFrameReader.\n",
    "\n",
    "**NOTE:** reading a set of small JSON files from HDFS is **very slow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from persons.json\n",
    "df = spark.read.load(\"persons.csv\",           # input file directory\n",
    "                        format = \"json\",       # input file fotmat\n",
    "                         multiLine = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-islam",
   "metadata": {},
   "source": [
    "## Creating DataFrames from RDDs or Python lists\n",
    "The content of an RDD of tuple or the content of a Python list of tuples can be stored in a DF by using:\n",
    "\n",
    "**spark.createDataFrame(data, schema)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "damaged-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python list of tuples\n",
    "profilesList = [(19, \"Justin\"), (30, \"Andy\"),(None, \"Michael\")]\n",
    "\n",
    "# Create a DataFrame from the profilesList\n",
    "df = spark.createDataFrame(profilesList,[\"age\",\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-queue",
   "metadata": {},
   "source": [
    "## Creating an RDD from a DataFrame\n",
    "The rdd member of the DataFrame class returns an RDD of Row objects containing the content of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from persons.csv (name, age)\n",
    "df = spark.read.load( \"persons.csv\",\n",
    "                        format=\"csv\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)\n",
    "\n",
    "# Define an RDD based on the content of the DataFrame\n",
    "rddRows = df.rdd\n",
    "\n",
    "# Use the map transformation to extract the name field/column\n",
    "rddNames = rddRows.map(lambda row: row.name)\n",
    "\n",
    "# Store the result\n",
    "rddNames.saveAsTextFile(outputPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
