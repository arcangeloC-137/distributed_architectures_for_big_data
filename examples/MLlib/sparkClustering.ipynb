{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satisfactory-appeal",
   "metadata": {},
   "source": [
    "# **Clustering**\n",
    "Spark MLlib provides a (limited) set of clustering algorithms\n",
    "- K-means\n",
    "- Bisecting k-means\n",
    "- Gaussian Mixture Model (GMM)\n",
    "\n",
    "Each clustering algorithm has its own parameters. However, all the provided algorithms identify a set of groups of objects/clusters and assign each input object to one single cluster All the clustering algorithms available in Spark work only with numerical data.\n",
    "\n",
    "The input of the MLlib clustering algorithms is a DataFrame containing a column called features of type Vector The clustering algorithm clusters the input records by considering only the content of features.\n",
    "\n",
    "## **Clustering with Mllib**\n",
    "1. Create a DataFrame with the features column\n",
    "2. Define the clustering pipeline and run the fit() method on the input data to infer the clustering model (e.g., the centroids of the k-means algorithm)\n",
    "    - This step returns a clustering model\n",
    "3. Invoke the transform() method of the inferred clustering model on the input data to assign each input record to a cluster\n",
    "    - This step returns a new DataFrame with the new column “prediction” in which the cluster identifier is stored for each input record\n",
    "    \n",
    "## **K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# input and output folders\n",
    "inputData = \"ex_datakmeans/dataClusteering.csv\"\n",
    "outputPath = \"clusterskmeans/“\n",
    "\n",
    "# Create a DataFrame from dataClusteering.csv\n",
    "# Training data in raw format\n",
    "inputDataDF = spark.read.load(inputData,\\\n",
    "                                format=\"csv\", header=True,\\\n",
    "                                inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an assembler to create a column (features) of type Vector\n",
    "# containing the double values associated with columns attr1, attr2, attr3\n",
    "assembler = VectorAssembler(inputCols=[\"attr1\", \"attr2\", \"attr3\"],\\\n",
    "outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-algebra",
   "metadata": {},
   "source": [
    "**Creation of the cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-means object.\n",
    "# k-means is an Estimator that is used to\n",
    "# create a k-means algorithm\n",
    "km = KMeans()\n",
    "# Set the value of k ( = number of clusters)\n",
    "km.setK(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline().setStages([assembler, km])\n",
    "\n",
    "# Execute the pipeline\n",
    "kmeansModel = pipeline.fit(inputDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the clustering model can be applied on the input data\n",
    "\n",
    "# to assign them to a cluster (i.e., assign a cluster id)\n",
    "# The returned DataFrame has the following schema (attributes)\n",
    "\n",
    "# - features: vector (values of the attributes)\n",
    "# - prediction: double (the predicted cluster id)\n",
    "# - original attributes attr1, attr2, attr3\n",
    "\n",
    "clusteredDataDF = kmeansModel.transform(inputDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the original columns and the clusterID (prediction) one\n",
    "# I rename prediction to clusterID\n",
    "clusteredData = clusteredDataDF\\\n",
    ".select(\"attr1\", \"attr2\", \"attr3\", \"prediction\")\\\n",
    ".withColumnRenamed(\"prediction\",\"clusterID\")\n",
    "\n",
    "# Save the result in an HDFS output folder\n",
    "clusteredData.write.csv(outputPath, header=\"true\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
