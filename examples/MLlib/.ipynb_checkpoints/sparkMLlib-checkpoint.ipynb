{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fantastic-guitar",
   "metadata": {},
   "source": [
    "### **Create Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifth-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Dense Vector\n",
    "dv = Vectors.dense([1.0,0.0,3.0])\n",
    "\n",
    "# Sparse\n",
    "sv = Vectors.sparse(3,{0:1.0, 2:3.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "linear-trout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 0.0, 3.0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "genetic-brunei",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 1.0, 2: 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-record",
   "metadata": {},
   "source": [
    "### **Create Matrixes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mysterious-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Matrices\n",
    "\n",
    "# Create a dense matrix with two rows and three columns\n",
    "# 3.0 0.0 0.0\n",
    "# 1.0 1.5 2.0\n",
    "dm = Matrices.dense(2,3,[3.0, 1.0, 0.0, 1.5, 0.0, 2.0])\n",
    "\n",
    "# Create a sparse version of the same matrix\n",
    "sm = Matrices.sparse(2,3, [0, 2, 3, 4], [0, 1, 1, 1] , [3,1,1.5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loose-sewing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(2, 3, [3.0, 1.0, 0.0, 1.5, 0.0, 2.0], False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adolescent-penguin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(2, 3, [0, 2, 3, 4], [0, 1, 1, 1], [3.0, 1.0, 1.5, 2.0], False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-divide",
   "metadata": {},
   "source": [
    "## **Vector Assembler**\n",
    "Is a transformer that combines a given list of columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# input and output folders\n",
    "inputPath = \"data/exampleDataAssembler.csv“\n",
    "\n",
    "# Create a DataFrame from the input data\n",
    "inputDF = spark.read.load(inputPath,\\\n",
    "format=\"csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a VectorAssembler that combines columns colB and colC\n",
    "# The new vetor column is called features\n",
    "myVectorAssembler = VectorAssembler(inputCols = ['colB', 'colC'],\\\n",
    "outputCol = 'features')\n",
    "\n",
    "# Apply myVectorAssembler on the input DataFrame\n",
    "transformedDF = myVectorAssembler.transform(inputDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-secondary",
   "metadata": {},
   "source": [
    "## **Standard Scaler**\n",
    "StandardScaler is an Estimator that returns a Transformer. StandardScalerModel transforms a vector column of an input DataFrame normalizing each “feature” of the input vector column to have unit standard deviation and/or zero mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# input and output folders\n",
    "inputPath = \"data/exampleDataAssembler.csv“\n",
    "\n",
    "# Create a DataFrame from the input data\n",
    "inputDF = spark.read.load(inputPath,\\\n",
    "format=\"csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a VectorAssembler that combines columns colB and colC\n",
    "# The new vetor column is called features\n",
    "myVectorAssembler = VectorAssembler(inputCols = ['colB', 'colC'],\\\n",
    "outputCol = 'features')\n",
    "\n",
    "# Apply myVectorAssembler on the input DataFrame\n",
    "transformedDF = myVectorAssembler.transform(inputDF)\n",
    "\n",
    "\n",
    "# Create a Standard Scaler to scale the content of features\n",
    "myScaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "\n",
    "# Before normalizing the content of the data we need to compute mean and\n",
    "\n",
    "# standard deviation of the analyzed data\n",
    "scalerModel = myScaler.fit(transformedDF)\n",
    "\n",
    "# Apply myScaler on the input column features\n",
    "scaledDF = scalerModel.transform(transformedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-newfoundland",
   "metadata": {},
   "source": [
    "## **String Indexer**\n",
    "Frequently the input data are characterized by categorical attributes. The Spark MLlib classification and regression algorithms work only with numerical values. Categorical columns must be mapped to double values.\n",
    "\n",
    "StringIndexerModel encodes a string column of “labels” to a column of “label indices”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coupled-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# input DataFrame\n",
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (3, \"c\"), (4, \"c\"), (5, \"a\")],\\\n",
    "[\"id\", \"category\"])\n",
    "\n",
    "# Create a StringIndexer to map the content of category to a set of “integers”\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Analyze the input data to define the mapping string -> integer\n",
    "indexerModel = indexer.fit(df)\n",
    "\n",
    "# Apply indexerModel on the input column category\n",
    "indexedDF = indexerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alone-european",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  1|       a|\n",
      "|  2|       b|\n",
      "|  3|       c|\n",
      "|  4|       c|\n",
      "|  5|       a|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "placed-festival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  1|       a|          0.0|\n",
      "|  2|       b|          2.0|\n",
      "|  3|       c|          1.0|\n",
      "|  4|       c|          1.0|\n",
      "|  5|       a|          0.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-canal",
   "metadata": {},
   "source": [
    "## IndexToString\n",
    "IndexToString, which is symmetrical to StringIndexer, is a Transformer that maps a column of “label indices” back to a column containing the original “labels” as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southern-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# input DataFrame\n",
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (3, \"c\"), (4, \"c\"), (5, \"a\")],\\\n",
    "[\"id\", \"category\"])\n",
    "\n",
    "# Create a StringIndexer to map the content of category to a set of “integers”\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Analyze the input data to define the mapping string -> integer\n",
    "indexerModel = indexer.fit(df)\n",
    "\n",
    "# Apply indexerModel on the input column category\n",
    "indexedDF = indexerModel.transform(df)\n",
    "\n",
    "\n",
    "# Create an IndexToString to map the content of numerical attribute categoryIndex\n",
    "# to the original string value\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\",\\\n",
    "labels=indexerModel.labels)\n",
    "\n",
    "# Apply converter on the input column categoryIndex\n",
    "reconvertedDF = converter.transform(indexedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prostate-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  1|       a|\n",
      "|  2|       b|\n",
      "|  3|       c|\n",
      "|  4|       c|\n",
      "|  5|       a|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vocational-reader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  1|       a|          0.0|\n",
      "|  2|       b|          2.0|\n",
      "|  3|       c|          1.0|\n",
      "|  4|       c|          1.0|\n",
      "|  5|       a|          0.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "several-vienna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+----------------+\n",
      "| id|category|categoryIndex|originalCategory|\n",
      "+---+--------+-------------+----------------+\n",
      "|  1|       a|          0.0|               a|\n",
      "|  2|       b|          2.0|               b|\n",
      "|  3|       c|          1.0|               c|\n",
      "|  4|       c|          1.0|               c|\n",
      "|  5|       a|          0.0|               a|\n",
      "+---+--------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reconvertedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-airplane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
