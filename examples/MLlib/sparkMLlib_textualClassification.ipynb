{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "institutional-bonus",
   "metadata": {},
   "source": [
    "# **Textual Data Classifiction** \n",
    "A set of preprocessing steps must be applied on the textual attribute before generating a classification model. Since Spark ML algorithms work only on “Tables” and double values, the textual part of the input data must be translated in a set of attributes to represent the data as a table. Many words are useless (e.g., conjunctions). Stopwords are usually removed. \n",
    "\n",
    "Traditionally a weight, based on the **TF-IDF** measure, is used to assign a difference importance to the words based on their frequency in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# input and output folders\n",
    "trainingData = \"ex_dataText/trainingData.csv\"\n",
    "unlabeledData = \"ex_dataText/unlabeledData.csv\"\n",
    "outputPath = \"predictionsLRPipelineText/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Training step\n",
    "# *************************\n",
    "\n",
    "# Create a DataFrame from trainingData.csv\n",
    "# Training data in raw format\n",
    "trainingData = spark.read.load(trainingData,\\\n",
    "                                format=\"csv\",\\\n",
    "                                header=True,\\\n",
    "                                inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-preference",
   "metadata": {},
   "source": [
    "### **Here the different part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of five stages:\n",
    "# tokenizer -> split sentences in set of words\n",
    "# remover -> remove stopwords\n",
    "# hashingTF -> map set of words to a fixed-length feature vectors (each\n",
    "# word becomes a feature and the value of the feature is the frequency of\n",
    "# the word in the sentence)\n",
    "# idf -> compute the idf component of the TF-IDF measure\n",
    "# lr -> logistic regression classification algorithm\n",
    "\n",
    "# The Tokenizer splits each sentence in a set of words.\n",
    "# It analyzes the content of column \"text\" and adds the\n",
    "# new column \"words\" in the returned DataFrame\n",
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords.\n",
    "# The StopWordsRemover component returns a new DataFrame with\n",
    "# a new column called \"filteredWords\". \"filteredWords\" is generated\n",
    "# by removing the stopwords from the content of column \"words\"\n",
    "\n",
    "remover = StopWordsRemover()\\\n",
    ".setInputCol(\"words\")\\\n",
    ".setOutputCol(\"filteredWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want now compute the TF-IDF (Term Frquency - Inverse Doc Frequency) component. \n",
    "# Unfortunately it doesn't exists a single method that automatically compute it\n",
    "# We need to pass through 2 Steps and 1 approximation\n",
    "\n",
    "# First Step: hashing TF\n",
    "# It maps each input word to a number and compute the \n",
    "# occurrencies of that number inside each sentence\n",
    "hashingTF = HashingTF()\\\n",
    ".setNumFeatures(1000)\\ # number of features of the vector (lucky if is > #words)\n",
    ".setInputCol(\"filteredWords\")\\\n",
    ".setOutputCol(\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the IDF transformation/computation.\n",
    "# System will compute the occurrences of each word in \n",
    "# the entire dataset\n",
    "idf = IDF()\\\n",
    ".setInputCol(\"rawFeatures\")\\\n",
    ".setOutputCol(\"features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification model based on the logistic regression algorithm\n",
    "# We can set the values of the parameters of the\n",
    "# Logistic Regression algorithm using the setter methods.\n",
    "lr = LogisticRegression()\\\n",
    ".setMaxIter(10)\\\n",
    ".setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([tokenizer, remover, hashingTF, idf, lr])\n",
    "\n",
    "# Execute the pipeline on the training data to build the\n",
    "# classification model\n",
    "classificationModel = pipeline.fit(trainingData)\n",
    "\n",
    "# Now, the classification model can be used to predict the class label\n",
    "# of new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Prediction step\n",
    "# *************************\n",
    "\n",
    "# Read unlabeled data\n",
    "# Create a DataFrame from unlabeledData.csv\n",
    "# Unlabeled data in raw format\n",
    "unlabeledData = spark.read.load(unlabeledData,\\\n",
    "    format=\"csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on unlabeled documents by using the\n",
    "# Transformer.transform() method.\n",
    "# The transform will only use the 'features' columns\n",
    "predictionsDF = classificationModel.transform(unlabeledData)\n",
    "\n",
    "# Select only the original features (i.e., the value of the original text attribute) and\n",
    "# the predicted class for each record\n",
    "predictions = predictionsDF.select(\"text\", \"prediction\"\n",
    "                                   \n",
    "# Save the result in an HDFS output folder\n",
    "predictions.write.csv(outputPath, header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-spyware",
   "metadata": {},
   "source": [
    "# **Sparse Labeled Data: The LIBSVM format**\n",
    "Frequently the training data are sparse. E.g., textual data are sparse. MLlib supports reading training examples stored in the LIBSVM format: it is a commonly used textual format that is used\n",
    "to represent sparse documents/data points.\n",
    "\n",
    "The LIBSVM format is a textual format in which each line represents an input record/data point by using a sparse feature vector.\n",
    "Each line has the format:\n",
    "- **label index1:value1 index2:value2 ...**\n",
    "\n",
    "Consider the following two records/data points characterized by 4 predictive features and a class label:\n",
    "- Features = [5.8, 1.7, 0 , 0 ] -- Label = 1\n",
    "- Features = [4.1, 0 , 2.5, 1.2] -- Label = 0\n",
    "Their LIBSVM format-based representation is the following:\n",
    "- 1 1:5.8 2:1.7\n",
    "- 0 1:4.1 3:2.5 4:1.2\n",
    "\n",
    "LIBSVM files can be loaded into DataFrames by combining the following methods:\n",
    "- read, format(\"libsvm\"), and load(inputpath)\n",
    "\n",
    "The returned DataFrame has two columns:\n",
    "- label: double\n",
    "    - The double value associated with the label\n",
    "- features: vector\n",
    "    - A sparse vector associated with the predictive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
    ".."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
