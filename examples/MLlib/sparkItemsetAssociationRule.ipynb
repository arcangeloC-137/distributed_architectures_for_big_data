{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "biological-faith",
   "metadata": {},
   "source": [
    "# **Itemset and Association Rule Mining**\n",
    "Spark MLlib provides:\n",
    "1. An itemset mining algorithm based on the FP-growth algorithm\n",
    "    - That extracts all the sets of items (of any length) with a minimum frequency\n",
    "2. A rule mining algorithm\n",
    "    - That extracts the association rules with a minimum frequency and a minimum confidence\n",
    "    - Only the rules with one single item in the consequent of the rules are extracted\n",
    "    \n",
    "FP-growth is one of the most popular and efficient itemset mining algorithms. It is characterized by one single parameter:\n",
    "- The minimum support threshold **(minsup)**\n",
    "The input dataset is a transactional dataset\n",
    "\n",
    "The input of the MLlib itemset and rule mining algorithm is a DataFrame containing a column called items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "confidential-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "# input and output folders\n",
    "transactionsData = \"./databases/transactions.csv\"\n",
    "outputPathItemsets = \"./Itemsets\"\n",
    "outputPathRules = \"./Rules\"\n",
    "\n",
    "# Create a DataFrame from transactions.csv\n",
    "transactionsDataDF = spark.read.load(transactionsData,\\\n",
    "                                        format=\"csv\", header=True,\\\n",
    "                                        inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "civil-capital",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transactions: string (nullable = true)\n",
      "\n",
      "+------------+\n",
      "|transactions|\n",
      "+------------+\n",
      "|     A B C D|\n",
      "|         A B|\n",
      "|         B C|\n",
      "|       A D E|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactionsDataDF.printSchema()\n",
    "transactionsDataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "nuclear-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Column transactions into an ArrayType\n",
    "trsDataDF = transactionsDataDF\\\n",
    ".selectExpr('split(transactions, \" \")')\\\n",
    ".withColumnRenamed(\"split(transactions,  )\", \"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "obvious-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------------+\n",
      "|       items|\n",
      "+------------+\n",
      "|[A, B, C, D]|\n",
      "|      [A, B]|\n",
      "|      [B, C]|\n",
      "|   [A, D, E]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trsDataDF.printSchema()\n",
    "trsDataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unnecessary-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an FP-growth Estimator\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
    "\n",
    "# Extract itemsets and rules\n",
    "model = fpGrowth.fit(trsDataDF)\n",
    "\n",
    "# Retrieve the DataFrame associated with the frequent itemsets\n",
    "dfItemsets = model.freqItemsets\n",
    "\n",
    "# Retrieve the DataFrame associated with the frequent rules\n",
    "dfRules = model.associationRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "brutal-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| items|freq|\n",
      "+------+----+\n",
      "|   [C]|   2|\n",
      "|[C, B]|   2|\n",
      "|   [A]|   3|\n",
      "|[A, B]|   2|\n",
      "|   [D]|   2|\n",
      "|[D, A]|   2|\n",
      "|   [B]|   3|\n",
      "+------+----+\n",
      "\n",
      "+----------+----------+------------------+------------------+\n",
      "|antecedent|consequent|        confidence|              lift|\n",
      "+----------+----------+------------------+------------------+\n",
      "|       [A]|       [B]|0.6666666666666666|0.8888888888888888|\n",
      "|       [A]|       [D]|0.6666666666666666|1.3333333333333333|\n",
      "|       [D]|       [A]|               1.0|1.3333333333333333|\n",
      "|       [B]|       [C]|0.6666666666666666|1.3333333333333333|\n",
      "|       [B]|       [A]|0.6666666666666666|0.8888888888888888|\n",
      "|       [C]|       [B]|               1.0|1.3333333333333333|\n",
      "+----------+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfItemsets.show()\n",
    "dfRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abandoned-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in an HDFS output folder\n",
    "dfItemsets.write.json(outputPathItemsets)\n",
    "\n",
    "# Save the result in an HDFS output folder\n",
    "dfRules.write.json(outputPathRules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-basin",
   "metadata": {},
   "source": [
    "The result is stored in a JSON file because itemsets and rules are stored in columns associated with the data type Array. Hence, CSV files cannot be used to store the result.\n",
    "\n",
    "**Itemset Output:**\n",
    "    \n",
    "    {\"items\":[\"C\"],\"freq\":2}\n",
    "    {\"items\":[\"C\",\"B\"],\"freq\":2}\n",
    "    {\"items\":[\"A\"],\"freq\":3}\n",
    "    {\"items\":[\"A\",\"B\"],\"freq\":2}\n",
    "    {\"items\":[\"D\"],\"freq\":2}\n",
    "    {\"items\":[\"D\",\"A\"],\"freq\":2}\n",
    "    {\"items\":[\"B\"],\"freq\":3}\n",
    "    \n",
    "**Rules Output:**\n",
    "\n",
    "    {\"antecedent\":[\"A\"],\"consequent\":[\"B\"],\"confidence\":0.6666666666666666,\"lift\":0.8888888888888888}\n",
    "    {\"antecedent\":[\"A\"],\"consequent\":[\"D\"],\"confidence\":0.6666666666666666,\"lift\":1.3333333333333333}\n",
    "    {\"antecedent\":[\"D\"],\"consequent\":[\"A\"],\"confidence\":1.0,\"lift\":1.3333333333333333}\n",
    "    {\"antecedent\":[\"B\"],\"consequent\":[\"C\"],\"confidence\":0.6666666666666666,\"lift\":1.3333333333333333}\n",
    "    {\"antecedent\":[\"B\"],\"consequent\":[\"A\"],\"confidence\":0.6666666666666666,\"lift\":0.8888888888888888}\n",
    "    {\"antecedent\":[\"C\"],\"consequent\":[\"B\"],\"confidence\":1.0,\"lift\":1.3333333333333333}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-source",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
