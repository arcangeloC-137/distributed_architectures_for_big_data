{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "willing-quebec",
   "metadata": {},
   "source": [
    "# **Performance Evaluation**\n",
    "In order to test the goodness of algorithms there are some evaluators\n",
    "The Evaluator can be:\n",
    "- a BinaryClassificationEvaluator for binary data\n",
    "- a MulticlassClassificationEvaluator for multiclass problems\n",
    "\n",
    "Provided metrics are:\n",
    "- **Accuracy**\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F-measure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-producer",
   "metadata": {},
   "source": [
    "The instantiated estimator has the method evaluate() that is applied on a DataFrame\n",
    "- It compares the predictions with the true label values\n",
    "- Output:\n",
    "    - The double value of the computed performance metric\n",
    "    \n",
    "Parameters of MulticlassClassificationEvaluator:\n",
    "- **metricName**\n",
    "    - ‘accuracy', ‘f1’, ‘weightedPrecision’, ‘weightedRecall’\n",
    "- **labelCol:input**\n",
    "    - Column with the true label/class value\n",
    "- **predictionCol:**\n",
    "    - Input column with the predicted class/label value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alone-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# input and output folders\n",
    "labeledData = \"./databases/trainingData.csv\"\n",
    "outputPath = \"./predictionsEval/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from labeledData.csv\n",
    "# Training data in raw format\n",
    "labeledDataDF = spark.read.load(labeledData,\\\n",
    "                                    format=\"csv\", header=True,\\\n",
    "                                    inferSchema=True)\n",
    "\n",
    "# Split labeled data in training and test set\n",
    "# training data : 75%\n",
    "# test data: 25%\n",
    "trainDF, testDF = labeledDataDF.randomSplit([0.75, 0.25], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valuable-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Training step\n",
    "# *************************\n",
    "\n",
    "# Define an assembler to create a column (features) of type Vector\n",
    "# containing the double values associated with columns attr1, attr2, attr3\n",
    "assembler = VectorAssembler(inputCols=[\"attr1\", \"attr2\", \"attr3\"],\\\n",
    "outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "synthetic-detail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_0532bab5fcbf"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a LogisticRegression object.\n",
    "lr = LogisticRegression()\n",
    "lr.setMaxIter(10)\n",
    "lr.setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecological-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline \n",
    "pipeline = Pipeline().setStages([assembler, lr])\n",
    "classificationModel = pipeline.fit(trainDF)\n",
    "\n",
    "# Now, the classification model can be used to predict the class label\n",
    "# of new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "delayed-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data using the transform() method of the\n",
    "# trained classification model transform uses only the content of 'features'\n",
    "# to perform the predictions.The model is associated with the pipeline and hence\n",
    "# also the assembler is executed\n",
    "predictionsDF = classificationModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "improving-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted value is column prediction\n",
    "# The actual label is column label\n",
    "# Define a set of evaluators\n",
    "\n",
    "myEvaluatorAcc = MulticlassClassificationEvaluator(labelCol=\"label\",\\\n",
    "                                                    predictionCol=\"prediction\",\\\n",
    "                                                    metricName='accuracy')\n",
    "\n",
    "myEvaluatorF1 = MulticlassClassificationEvaluator(labelCol=\"label\",\\\n",
    "                                                    predictionCol=\"prediction\",\\\n",
    "                                                    metricName='f1')\n",
    "\n",
    "myEvaluatorWeightedPrecision = MulticlassClassificationEvaluator(labelCol=\"label\",\\\n",
    "                                                            predictionCol=\"prediction\",\\\n",
    "                                                                metricName='weightedPrecision')\n",
    "myEvaluatorWeightedRecall = MulticlassClassificationEvaluator(labelCol=\"label\",\\\n",
    "                                                            predictionCol=\"prediction\",\\\n",
    "                                                                metricName='weightedRecall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exterior-treasurer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data  1.0\n",
      "F1 on test data  1.0\n",
      "Weighted recall on test data  1.0\n",
      "Weighted precision on test data  1.0\n"
     ]
    }
   ],
   "source": [
    "# Apply the evaluators on the predictions associated with the test data\n",
    "# Print the results on the standard output\n",
    "\n",
    "print(\"Accuracy on test data \", myEvaluatorAcc.evaluate(predictionsDF))\n",
    "print(\"F1 on test data \", myEvaluatorF1.evaluate(predictionsDF))\n",
    "print(\"Weighted recall on test data \",\\\n",
    "                    myEvaluatorWeightedRecall.evaluate(predictionsDF))\n",
    "print(\"Weighted precision on test data \",\\\n",
    "                    myEvaluatorWeightedPrecision.evaluate(predictionsDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-links",
   "metadata": {},
   "source": [
    "### Do you think caching is useful here?\n",
    "Do you think it would be useful to cache the trainig set, or something else? In this case we use the matrix associated with the predictions many times. For this reason it would be a good idea to cache its content. \n",
    "\n",
    "Is also a good idea to cache the training DF. If we analyse the content of the algorithm we are going to apply, for instance the LR, the system will iterate many times on the training DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-feelings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
