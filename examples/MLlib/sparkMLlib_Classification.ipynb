{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "english-travel",
   "metadata": {},
   "source": [
    "# **Classification Algorithms**\n",
    "Spark MLlib provides a (limited) set of classification algorithms\n",
    "- Logistic regression\n",
    "    - Binomial logistic regression\n",
    "    - Multinomial logistic regression\n",
    "- Decision tree classifier\n",
    "- Random forest classifier\n",
    "- Gradient-boosted tree classifier\n",
    "- Multilayer perceptron classifier\n",
    "- Linear Support Vector Machine\n",
    "\n",
    "All the Spark classification algorithms are trained on top of an input DataFrame containing (at least) two columns:\n",
    "- label:\n",
    "    - The class label, i.e., the attribute to be predicted by the classification model\n",
    "    - It is an integer value (casted to a double)\n",
    "- features:\n",
    "    - A vector of doubles containing the values of the predictive attributes of the input records/data points\n",
    "    - The data type of this column is pyspark.ml.linalg.Vector\n",
    "    - Both dense and sparse vectors can be use \n",
    "    \n",
    "## **Logistic Regression**\n",
    "\n",
    "Consider the following example input training data file\n",
    "\n",
    "label, attr1, attr2, attr3\n",
    "\n",
    "  1.0,   0.0,   1.1,   0.1\n",
    "\n",
    "  0.0,   2.0,   1.0,  -1.0\n",
    "\n",
    "  0.0,   2.0,   1.3,   1.0\n",
    "\n",
    "  1.0,   0.0,   1.2,  -0.5\n",
    "  \n",
    "It contains four records/data points. This is a binary classification problem because\n",
    "the class label assumes only two values: **0 and 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "double-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# input and output folders\n",
    "trainingData = \"./databases/trainingData.csv\"\n",
    "unlabeledData = \"./databases/unlabeledData.csv\"\n",
    "outputPath = \"./predictionsLR/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defined-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Training step\n",
    "# *************************\n",
    "\n",
    "# Create a DataFrame from trainingData.csv\n",
    "# Training data in raw format\n",
    "trainingData = spark.read.load(trainingData,\\\n",
    "                                format=\"csv\",\\\n",
    "                                header=True,\\\n",
    "                                inferSchema=True)\n",
    "\n",
    "# Define an assembler to create a column (features) of type Vector\n",
    "# containing the double values associated with columns attr1, attr2, attr3\n",
    "assembler = VectorAssembler(inputCols=[\"attr1\", \"attr2\", \"attr3\"],\\\n",
    "outputCol=\"features\")\n",
    "\n",
    "# Apply the assembler to create column features for the training data\n",
    "trainingDataDF = assembler.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reasonable-appearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+--------------+\n",
      "|label|attr1|attr2|attr3|      features|\n",
      "+-----+-----+-----+-----+--------------+\n",
      "|  1.0|  0.0|  1.1|  0.1| [0.0,1.1,0.1]|\n",
      "|  0.0|  2.0|  1.0| -1.0|[2.0,1.0,-1.0]|\n",
      "|  0.0|  2.0|  1.3|  1.0| [2.0,1.3,1.0]|\n",
      "|  1.0|  0.0|  1.2| -0.5|[0.0,1.2,-0.5]|\n",
      "+-----+-----+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingDataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantitative-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LogisticRegression object.\n",
    "# LogisticRegression is an Estimator that is used to\n",
    "# create a classification model based on logistic regression.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# We can set the values of the parameters of the\n",
    "# Logistic Regression algorithm using the setter methods.\n",
    "# There is one set method for each parameter\n",
    "# For example, we are setting the number of maximum iterations to 10\n",
    "# and the regularization parameter. to 0.0.1\n",
    "lr.setMaxIter(10)\n",
    "lr.setRegParam(0.01)\n",
    "\n",
    "# Train a logistic regression model on the training data\n",
    "classificationModel = lr.fit(trainingDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sized-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Prediction step\n",
    "# *************************\n",
    "\n",
    "# Create a DataFrame from unlabeledData.csv\n",
    "# Unlabeled data in raw format\n",
    "unlabeledData = spark.read.load(unlabeledData,\\\n",
    "format=\"csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Apply the same assembler we created before also on the unlabeled data\n",
    "# to create the features column\n",
    "unlabeledDataDF = assembler.transform(unlabeledData)\n",
    "\n",
    "# Make predictions on the unlabled data using the transform() method of the\n",
    "# trained classification model transform uses only the content of 'features'\n",
    "# to perform the predictions\n",
    "predictionsDF = classificationModel.transform(unlabeledDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "urban-white",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|attr1|attr2|attr3|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+-----+-----+-----+--------------+--------------------+--------------------+----------+\n",
      "| null| -1.0|  1.5|  1.3|[-1.0,1.5,1.3]|[-6.5872014439355...|[0.00137599470692...|       1.0|\n",
      "| null|  3.0|  2.0| -0.1|[3.0,2.0,-0.1]|[3.98018281942565...|[0.98166040093741...|       0.0|\n",
      "| null|  0.0|  2.2| -1.5|[0.0,2.2,-1.5]|[-6.3765177028604...|[0.00169814755783...|       1.0|\n",
      "+-----+-----+-----+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned DataFrame has the following schema (attributes)\n",
    "# - attr1\n",
    "# - attr2\n",
    "# - attr3\n",
    "# - features: vector (values of the attributes)\n",
    "# - label: double (value of the class label)\n",
    "# - rawPrediction: vector (nullable = true)\n",
    "# - probability: vector (The i-th cell contains the probability that the current\n",
    "# record belongs to the i-th class\n",
    "# - prediction: double (the predicted class label)\n",
    "# Select only the original features (i.e., the value of the original attributes\n",
    "# attr1, attr2, attr3) and the predicted class for each record\n",
    "predictions = predictionsDF.select(\"attr1\", \"attr2\", \"attr3\", \"prediction\")\n",
    "\n",
    "# Save the result in an HDFS output folder\n",
    "predictions.write.csv(outputPath, header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "severe-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+----------+\n",
      "|attr1|attr2|attr3|prediction|\n",
      "+-----+-----+-----+----------+\n",
      "| -1.0|  1.5|  1.3|       1.0|\n",
      "|  3.0|  2.0| -0.1|       0.0|\n",
      "|  0.0|  2.2| -1.5|       1.0|\n",
      "+-----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fifty-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only predictions with prob of class 1 > 0.9\n",
    "\n",
    "from pyspark.sql.types import DoubleType \n",
    "\n",
    "def extractProb(probabilities, labelIndex):\n",
    "    return float(probabilities[labelIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lyric-registrar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extractProb(probabilities, labelIndex)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability is a vector. I want to select the lines\n",
    "# where the second element of this vector is greater than 0.9\n",
    "\n",
    "spark.udf.register(\"extractProb\", extractProb, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caroline-reflection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- P0: double (nullable = true)\n",
      " |-- P1: double (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+------------------------------------------+---------------------+-------------------+----------+\n",
      "|probability                               |P0                   |P1                 |prediction|\n",
      "+------------------------------------------+---------------------+-------------------+----------+\n",
      "|[0.0013759947069214356,0.9986240052930786]|0.0013759947069214356|0.9986240052930786 |1.0       |\n",
      "|[0.9816604009374171,0.01833959906258293]  |0.9816604009374171   |0.01833959906258293|0.0       |\n",
      "|[0.0016981475578358176,0.9983018524421641]|0.0016981475578358176|0.9983018524421641 |1.0       |\n",
      "+------------------------------------------+---------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF = predictionsDF.selectExpr(\"probability\",\\\n",
    "                                \"extractProb(probability, 0) as P0\",\n",
    "                                \"extractProb(probability, 1) as P1\",\n",
    "                                \"prediction\")\n",
    "tempDF.printSchema()\n",
    "tempDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-vinyl",
   "metadata": {},
   "source": [
    "highProbDF = predictionsDF.filter(\"extractProb(probability,1)>0.9\")\n",
    "highProbDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to store the model\n",
    "# prameter -> folder to store the model\n",
    "classificationModel.save(\"modelLR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suffering-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will create two folders:\n",
    "# - metadata: description of the model and its configuration\n",
    "# - data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-potato",
   "metadata": {},
   "source": [
    "## **Logistic Regression with Pipelines**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "western-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# input and output folders\n",
    "trainingData = \"./databases/trainingData.csv\"\n",
    "unlabeledData = \"./databases/unlabeledData.csv\"\n",
    "outputPath = \"./predictionsLR/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "united-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "\n",
    "# *************************\n",
    "# Training step\n",
    "# *************************\n",
    "# Create a DataFrame from trainingData.csv\n",
    "# Training data in raw format\n",
    "trainingData = spark.read.load(trainingData,\\\n",
    "        format=\"csv\",\\\n",
    "        header=True,\\\n",
    "        inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "precious-avatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_86931dac514d"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define an assembler to create a column (features) of type Vector\n",
    "# containing the double values associated with columns attr1, attr2, attr3\n",
    "assembler = VectorAssembler(inputCols=[\"attr1\", \"attr2\", \"attr3\"], outputCol=\"features\")\n",
    "\n",
    "# Create a LogisticRegression object\n",
    "# LogisticRegression is an Estimator that is used to\n",
    "# create a classification model based on logistic regression.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# We can set the values of the parameters of the\n",
    "# Logistic Regression algorithm using the setter methods.\n",
    "# There is one set method for each parameter\n",
    "# For example, we are setting the number of maximum iterations to 10\n",
    "# and the regularization parameter. to 0.0.1\n",
    "lr.setMaxIter(10)\n",
    "lr.setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sustained-scenario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorAssembler_130260aa10ff\n"
     ]
    }
   ],
   "source": [
    "print(assembler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-combat",
   "metadata": {},
   "source": [
    "Now we can create our **pipeline**. The parameter of **.setStages(...)** is a vector of steps. First we want to apply the assembler step, then we want to perform the Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can define our Pipeline\n",
    "# The pipeline includes also the preprocessing step\n",
    "pipeline = Pipeline().setStages([assembler, lr])\n",
    "\n",
    "# Execute the pipeline on the training data to build the\n",
    "# classification model\n",
    "classificationModel = pipeline.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-grill",
   "metadata": {},
   "source": [
    "Now, the classification model can be used to predict the class label of new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sunset-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Prediction step\n",
    "# *************************\n",
    "# Create a DataFrame from unlabeledData.csv\n",
    "# Unlabeled data in raw format\n",
    "unlabeledData = spark.read.load(unlabeledData, format=\"csv\", header=True, inferSchema=True)\n",
    "\n",
    "# We simply apply transform to the classification model, giving the unlabled data as input\n",
    "predictions = classificationModel.transform(unlabeledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "medical-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned DataFrame has the following schema (attributes)\n",
    "# - attr1\n",
    "# - attr2\n",
    "# - attr3\n",
    "# - features: vector (values of the attributes)\n",
    "# - label: double (value of the class label)\n",
    "# - rawPrediction: vector (nullable = true)\n",
    "# - probability: vector (The i-th cell contains the probability that the current\n",
    "# record belongs to the i-th class\n",
    "# - prediction: double (the predicted class label)\n",
    "# Select only the original features (i.e., the value of the original attributes\n",
    "# attr1, attr2, attr3) and the predicted class for each record\n",
    "\n",
    "predictions = predictionsDF.select(\"attr1\", \"attr2\", \"attr3\", \"prediction\")\n",
    "\n",
    "# Save the result in an HDFS output folder\n",
    "# predictions.write.csv(outputPath, header=\"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "collaborative-enclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|label|attr1|attr2|attr3|\n",
      "+-----+-----+-----+-----+\n",
      "|  1.0|  0.0|  1.1|  0.1|\n",
      "|  0.0|  2.0|  1.0| -1.0|\n",
      "|  0.0|  2.0|  1.3|  1.0|\n",
      "|  1.0|  0.0|  1.2| -0.5|\n",
      "+-----+-----+-----+-----+\n",
      "\n",
      "+-----+-----+-----+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|attr1|attr2|attr3|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+-----+-----+-----+--------------+--------------------+--------------------+----------+\n",
      "| null| -1.0|  1.5|  1.3|[-1.0,1.5,1.3]|[-6.5872014439355...|[0.00137599470692...|       1.0|\n",
      "| null|  3.0|  2.0| -0.1|[3.0,2.0,-0.1]|[3.98018281942565...|[0.98166040093741...|       0.0|\n",
      "| null|  0.0|  2.2| -1.5|[0.0,2.2,-1.5]|[-6.3765177028604...|[0.00169814755783...|       1.0|\n",
      "+-----+-----+-----+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show()\n",
    "predictionsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-legislature",
   "metadata": {},
   "source": [
    "## **Decision Tree**\n",
    "Same solution as previous, just change one line of code: **algorithm name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "north-caution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "balanced-sitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier_56504325f349"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DecisionTreeClassifier object.\n",
    "# DecisionTreeClassifier is an Estimator that is used to\n",
    "# create a classification model based on decision trees.\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# We can set the values of the parameters of the DecisionTree\n",
    "# For example we can set the measure that is used to decide if a\n",
    "# node must be split. In this case we set gini index\n",
    "dt.setImpurity(\"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
