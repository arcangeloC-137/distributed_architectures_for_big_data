{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accessory-ferry",
   "metadata": {},
   "source": [
    "### **Create Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "injured-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Dense Vector\n",
    "dv = Vectors.dense([1.0,0.0,3.0])\n",
    "\n",
    "# Sparse\n",
    "sv = Vectors.sparse(3,{0:1.0, 2:3.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tight-movement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 0.0, 3.0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wanted-perfume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 1.0, 2: 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-cursor",
   "metadata": {},
   "source": [
    "### **Create Matrixes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "remarkable-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Matrices\n",
    "\n",
    "# Create a dense matrix with two rows and three columns\n",
    "# 3.0 0.0 0.0\n",
    "# 1.0 1.5 2.0\n",
    "dm = Matrices.dense(2,3,[3.0, 1.0, 0.0, 1.5, 0.0, 2.0])\n",
    "\n",
    "# Create a sparse version of the same matrix\n",
    "sm = Matrices.sparse(2,3, [0, 2, 3, 4], [0, 1, 1, 1] , [3,1,1.5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "significant-knowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(2, 3, [3.0, 1.0, 0.0, 1.5, 0.0, 2.0], False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ultimate-morgan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(2, 3, [0, 2, 3, 4], [0, 1, 1, 1], [3.0, 1.0, 1.5, 2.0], False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-design",
   "metadata": {},
   "source": [
    "## **Vector Assembler**\n",
    "Is a transformer that combines a given list of columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# input and output folders\n",
    "inputPath = \"data/exampleDataAssembler.csv“\n",
    "\n",
    "# Create a DataFrame from the input data\n",
    "inputDF = spark.read.load(inputPath,\\\n",
    "format=\"csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a VectorAssembler that combines columns colB and colC\n",
    "# The new vetor column is called features\n",
    "myVectorAssembler = VectorAssembler(inputCols = ['colB', 'colC'],\\\n",
    "outputCol = 'features')\n",
    "\n",
    "# Apply myVectorAssembler on the input DataFrame\n",
    "transformedDF = myVectorAssembler.transform(inputDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-pattern",
   "metadata": {},
   "source": [
    "## **Standard Scaler**\n",
    "StandardScaler is an Estimator that returns a Transformer. StandardScalerModel transforms a vector column of an input DataFrame normalizing each “feature” of the input vector column to have unit standard deviation and/or zero mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# input and output folders\n",
    "inputPath = \"data/exampleDataAssembler.csv“\n",
    "\n",
    "# Create a DataFrame from the input data\n",
    "inputDF = spark.read.load(inputPath,\\\n",
    "format=\"csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a VectorAssembler that combines columns colB and colC\n",
    "# The new vetor column is called features\n",
    "myVectorAssembler = VectorAssembler(inputCols = ['colB', 'colC'],\\\n",
    "outputCol = 'features')\n",
    "\n",
    "# Apply myVectorAssembler on the input DataFrame\n",
    "transformedDF = myVectorAssembler.transform(inputDF)\n",
    "\n",
    "\n",
    "# Create a Standard Scaler to scale the content of features\n",
    "myScaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "\n",
    "# Before normalizing the content of the data we need to compute mean and\n",
    "\n",
    "# standard deviation of the analyzed data\n",
    "scalerModel = myScaler.fit(transformedDF)\n",
    "\n",
    "# Apply myScaler on the input column features\n",
    "scaledDF = scalerModel.transform(transformedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-lecture",
   "metadata": {},
   "source": [
    "## **String Indexer**\n",
    "Frequently the input data are characterized by categorical attributes. The Spark MLlib classification and regression algorithms work only with numerical values. Categorical columns must be mapped to double values.\n",
    "\n",
    "StringIndexerModel encodes a string column of “labels” to a column of “label indices”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "matched-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# input DataFrame\n",
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (3, \"c\"), (4, \"c\"), (5, \"a\")],\\\n",
    "[\"id\", \"category\"])\n",
    "\n",
    "# Create a StringIndexer to map the content of category to a set of “integers”\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Analyze the input data to define the mapping string -> integer\n",
    "indexerModel = indexer.fit(df)\n",
    "\n",
    "# Apply indexerModel on the input column category\n",
    "indexedDF = indexerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imperial-rachel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  1|       a|\n",
      "|  2|       b|\n",
      "|  3|       c|\n",
      "|  4|       c|\n",
      "|  5|       a|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "emotional-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  1|       a|          0.0|\n",
      "|  2|       b|          2.0|\n",
      "|  3|       c|          1.0|\n",
      "|  4|       c|          1.0|\n",
      "|  5|       a|          0.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-gender",
   "metadata": {},
   "source": [
    "## **IndexToString**\n",
    "IndexToString, which is symmetrical to StringIndexer, is a Transformer that maps a column of “label indices” back to a column containing the original “labels” as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "young-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# input DataFrame\n",
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (3, \"c\"), (4, \"c\"), (5, \"a\")],\\\n",
    "[\"id\", \"category\"])\n",
    "\n",
    "# Create a StringIndexer to map the content of category to a set of “integers”\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Analyze the input data to define the mapping string -> integer\n",
    "indexerModel = indexer.fit(df)\n",
    "\n",
    "# Apply indexerModel on the input column category\n",
    "indexedDF = indexerModel.transform(df)\n",
    "\n",
    "\n",
    "# Create an IndexToString to map the content of numerical attribute categoryIndex\n",
    "# to the original string value\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\",\\\n",
    "labels=indexerModel.labels)\n",
    "\n",
    "# Apply converter on the input column categoryIndex\n",
    "reconvertedDF = converter.transform(indexedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crucial-penalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  1|       a|\n",
      "|  2|       b|\n",
      "|  3|       c|\n",
      "|  4|       c|\n",
      "|  5|       a|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "educated-techno",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  1|       a|          0.0|\n",
      "|  2|       b|          2.0|\n",
      "|  3|       c|          1.0|\n",
      "|  4|       c|          1.0|\n",
      "|  5|       a|          0.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "central-consequence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+----------------+\n",
      "| id|category|categoryIndex|originalCategory|\n",
      "+---+--------+-------------+----------------+\n",
      "|  1|       a|          0.0|               a|\n",
      "|  2|       b|          2.0|               b|\n",
      "|  3|       c|          1.0|               c|\n",
      "|  4|       c|          1.0|               c|\n",
      "|  5|       a|          0.0|               a|\n",
      "+---+--------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reconvertedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-plaintiff",
   "metadata": {},
   "source": [
    "### **SQL Transformer**\n",
    "Is used to perform an SQL query on an Input DF to obtain an output DF. SQLTransformer is a transformer that implements the transformations which are defined by SQL queries\n",
    "- Currently, the syntax of the supported (simplified) SQL queries is\n",
    "    - “SELECT attributes, function(attributes) \n",
    "        FROM \\__THIS\\__”\n",
    "- __THIS__ represents the DataFrame on which the SQLTransformer is invoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prospective-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "#Local Input data\n",
    "inputList = [(1, \"This is Spark\"),\\\n",
    "(2, \"Spark\"),\\\n",
    "(3, \"Another sample sentence of words\"),\\\n",
    "(4, \"Paolo Rossi\"),\\\n",
    "(5, \"Giovanni\")]\n",
    "\n",
    "# Create the initial DataFrame\n",
    "dfInput = spark.createDataFrame(inputList, [\"id\", \"text\"])\n",
    "\n",
    "# Define a UDF function that that counts the number of words in an input string\n",
    "spark.udf.register(\"countWords\", lambda text: len(text.split(\" \")), IntegerType())\n",
    "\n",
    "# Define an SQLTranformer to create the columns we are interested in\n",
    "sqlTrans = SQLTransformer(statement=\"\"\"SELECT *,\n",
    "                                        countWords(text) AS numLines\n",
    "                                       FROM __THIS__\"\"\")\n",
    "\n",
    "# Create the new DataFrame by invoking the transform method of the\n",
    "# defined SQLTranformer\n",
    "newDF = sqlTrans.transform(dfInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "domestic-military",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                text|\n",
      "+---+--------------------+\n",
      "|  1|       This is Spark|\n",
      "|  2|               Spark|\n",
      "|  3|Another sample se...|\n",
      "|  4|         Paolo Rossi|\n",
      "|  5|            Giovanni|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfInput.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "patient-spray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+\n",
      "| id|                text|numLines|\n",
      "+---+--------------------+--------+\n",
      "|  1|       This is Spark|       3|\n",
      "|  2|               Spark|       1|\n",
      "|  3|Another sample se...|       5|\n",
      "|  4|         Paolo Rossi|       2|\n",
      "|  5|            Giovanni|       1|\n",
      "+---+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-december",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
