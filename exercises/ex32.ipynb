{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handled-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 32 - Maximum Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "signed-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a collection of structured textual csv files containing the daily value of PM10 for a set of sensors\n",
    "# Output: report the maximum value of PM10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wound-canada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.42.19.248:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0-cdh6.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accomplished-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"/data/students/bigdata-01QYD/ex_data/Ex32/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "direct-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file\n",
    "# Each line of the files has the following format: (sensorId, date, PM10 value \\n)\n",
    "#logRDD = sc.textFile(inputPath)\n",
    "\n",
    "# I can also decide the number of partitions to put\n",
    "logRDD = sc.textFile(inputPath,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sharp-remains",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s1,2016-01-01,20.5',\n",
       " 's2,2016-01-02,30.1',\n",
       " 's1,2016-01-01,60.2',\n",
       " 's2,2016-01-02,20.4',\n",
       " 's1,2016-01-03,55.5',\n",
       " 's2,2016-01-03,52.5']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "indoor-sword",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s2,2016-01-03,52.5', 's2,2016-01-02,30.1', 's2,2016-01-02,20.4']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the file was too big for collect:\n",
    "logRDD.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "verbal-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmRDD = logRDD.map(lambda logline: logline.split(',')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "numerous-waste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60.2'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmRDD.top(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "executive-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or with Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "coral-howard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.2\n"
     ]
    }
   ],
   "source": [
    "maxPM10Value = pmRDD.reduce(lambda v1,v2: max(v1,v2))\n",
    "print(maxPM10Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "enclosed-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function print some statistics about the input RDD\n",
    "def analyzePartitions(myRDD):\n",
    "    if myRDD.partitioner is None:\n",
    "        print(\"Partitioner: No partitioner\")\n",
    "    else:\n",
    "        print(\"Partitinoer: \"+str(myRDD.partitioner.partitionFunc()))\n",
    "        \n",
    "    print(\"Num. partitions: \"+str(myRDD.getNumPartitions()))\n",
    "    \n",
    "    # Create a local copu of the input partitions in a local Python list\n",
    "    partitions = myRDD.glom().collect()\n",
    "    \n",
    "    print(\"Content of the partitions\")\n",
    "    \n",
    "    for p in partitions:\n",
    "        print(str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "linear-billy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioner: No partitioner\n",
      "Num. partitions: 7\n",
      "Content of the partitions\n",
      "['s1,2016-01-01,20.5']\n",
      "['s2,2016-01-02,30.1']\n",
      "['s1,2016-01-01,60.2']\n",
      "['s2,2016-01-02,20.4']\n",
      "['s1,2016-01-03,55.5']\n",
      "['s2,2016-01-03,52.5']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "analyzePartitions(logRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "velvet-consumer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7) PythonRDD[17] at RDD at PythonRDD.scala:53 []\n",
      " |  /data/students/bigdata-01QYD/ex_data/Ex32/data/ MapPartitionsRDD[12] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  /data/students/bigdata-01QYD/ex_data/Ex32/data/ HadoopRDD[11] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(pmRDD.toDebugString().decode()) # first number is the num of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-tracker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
